[["index.html", "STAT 720 - Design of Experiments Day 1 Welcome to STAT 720! 1.1 About this course: 1.2 Learning goals 1.3 On notation 1.4 Why do designed experiments exist? 1.5 The golden rules of designed experiments 1.6 Other concepts in designed experiments 1.7 For tomorrow", " STAT 720 - Design of Experiments Josefina Lacasa Summer 2025 Day 1 Welcome to STAT 720! June 9th, 2025 1.1 About this course: About me About you: library(tidyverse) read.csv(&quot;../../students_STAT_720_C.csv&quot;) %&gt;% ggplot(aes(x = degreeProgram))+ geom_bar(fill = &quot;#B388EB&quot;)+ theme_bw()+ theme(panel.grid.minor = element_blank(), panel.border = element_blank(), axis.title.x = element_blank(), axis.text.x = element_text(angle = 50, vjust = 1, hjust=1)) In rounds: What’s your major, what do you expect to learn? 1.1.1 Logistics Website Syllabus Statistical programming requirements Rough mindmap of the course (on whiteboard) Semester project - design your own experiment. Grades: A (100-89.999999999(!!!)), B (89.99-79.99), C (79.99-69.99), D (69.99-59.99), F (&lt;59.99). Attendance policies Semester projects 1.2 Learning goals By the end of this course, you should be able to: - Be able to identify the treatment design, experiment design, experimental unit and observational unit. - Be able to write the statistical model that corresponds to (simple) designed experiments. - Be able to write the Materials and Methods section in a paper (or thesis) that describes the designed experiment. - Distinguish the benefits/disadvantages of different experiment designs. 1.3 On notation scalars: \\(y\\), \\(\\sigma\\), \\(\\beta_0\\) vectors: \\(\\mathbf{y} \\equiv [y_1, y_2, ..., y_n]&#39;\\), \\(\\boldsymbol{\\beta} \\equiv [\\beta_1, \\beta_2, ..., \\beta_p]&#39;\\), \\(\\boldsymbol{u}\\) matrices: \\(\\mathbf{X}\\), \\(\\Sigma\\) probability distribution: \\(y \\sim N(0, \\sigma^2)\\), \\(\\mathbf{y} \\sim N(\\boldsymbol{0}, \\sigma^2\\mathbf{I})\\). 1.4 Why do designed experiments exist? 1.4.1 Example You want to bake cookies with a certain diameter (to fit the box) and are not sure about the amount of baking powder vs. baking soda. more baking powder = smaller, more cakey cookies more baking soda = larger, crispier cookies How can we estimate the cookie diameter Case A: Bake 3 cookies, each with a different levels of baking powder:baking soda ratio. Case B: - On Monday: Bake 3 cookies, each with a different levels of baking powder:baking soda ratio. - On Tuesday: Bake 3 cookies, each with a different levels of baking powder:baking soda ratio. - On Wednesday: Bake 3 cookies, each with a different levels of baking powder:baking soda ratio. Case C: - On Monday: Bake 3 cookies with the first level of baking powder:baking soda ratio. - On Tuesday: Bake 3 cookies with the second level of baking powder:baking soda ratio. - On Wednesday: Bake 3 cookies with the third level of baking powder:baking soda ratio. Group discussion: which is preferrable? 1.5 The golden rules of designed experiments Randomization Replication Local control 1.6 Other concepts in designed experiments Experimental unit: smallest unit to which a treatment is independently assigned/applied. Observational unit: smallest unit on which observations are made. 1.7 For tomorrow Install R and RStudio. "],["basic-types-of-designed-experiments.html", "Day 2 Basic types of designed experiments 2.1 Announcements 2.2 Review 2.3 Types of designs - the basics 2.4 Skeleton ANOVA tables 2.5 Homeworks", " Day 2 Basic types of designed experiments June 10th, 2025 2.1 Announcements Not sure about your project topic? Schedule a meeting! 2.2 Review Differences between observational data and data generated by controlled experiments. The role of experiment design in causal inference. The golden rules of designed experiments: Randomization Replication Local control (blocking) Experimental unit: smallest unit to which a treatment is independently assigned/applied. Observational unit: smallest unit on which observations are made. 2.3 Types of designs - the basics There are several ways to carry out an experiment. The way we carry out an experiment is important because it will establish the blueprint for how the data are generated. This means that the design will determine what observations are similar to each other. Figure 2.1: A blueprint is to a building what the experimental design is to the data. Source 2.3.1 Completely randomized design (CRD) Blueprint: all experimental units are similar. The only factor driving differences between observations is the treatment (and randomness). \\[\\begin{equation} y_{ij} = \\mu + \\tau_i + \\varepsilon_{ij}, \\end{equation}\\] \\[\\begin{equation} \\varepsilon_{ij} \\sim N(0, \\sigma^2), \\end{equation}\\] where \\(y_{ij}\\) is the \\(j\\)th observation of the \\(i\\)th treatment, \\(\\mu\\) is the overall mean, \\(\\tau_i\\) is the effect of the \\(i\\)th treatment, and \\(\\varepsilon_{ij}\\) is the residual for the \\(j\\)th observation of the \\(i\\)th treatment (i.e., the difference between observed and predicted). Figure 2.2: Schematic description of an experiment with a completely randomized design 2.3.2 Randomized complete block design (RCBD) Blocks are groups of similar experimental units and are large enough to fit each treatment at least once Blueprint: all experimental from the same block are similar to each other. The factors driving differences between observations are the treatment and the blocks (and randomness). \\[\\begin{equation} y_{ijk} = \\mu + \\tau_i + \\rho_j + \\varepsilon_{ijk}, \\end{equation}\\] \\[\\begin{equation} \\varepsilon_{ijk} \\sim N(0, \\sigma^2), \\end{equation}\\] where \\(y_{ij}\\) is the \\(j\\)th observation of the \\(i\\)th treatment, \\(\\mu\\) is the overall mean, \\(\\tau_i\\) is the effect of the \\(i\\)th treatment, \\(\\rho_j\\) is the effect of the \\(j\\)th block, and \\(\\varepsilon_{ij}\\) is the residual for the \\(j\\)th observation of the \\(i\\)th treatment (i.e., the difference between observed and predicted). Figure 2.3: Schematic description of an experiment with a randomized complete block design 2.3.3 Incomplete block design (IBD) Blocks are groups of similar experimental units and are not large enough to fit each treatment at least once Blueprint: all experimental from the same block are similar to each other. The factors driving differences between observations are the treatment and the blocks (and randomness). We will define the model for this design later on, after learning more about mixed models. Figure 2.4: Schematic description of an experiment with an incomplete block design 2.4 Skeleton ANOVA tables The Analysis of Variance (ANOVA) can be considered a special case of a linear model that divides the predictors in bins to evaluate their influence on the response (Gelman 2005). Write the topographical sources of variation Write the treatment sources of variation Degrees of freedom An intuitive approach to degrees of freedom: number of independent values. Whiteboard demo. Combine the combined skeleton ANOVA. 2.4.1 Practice examples Plant breeding. A field has 4 homogeneous regions and is divided into 18 smaller plots. Withing those regions, the researchers have randomly assigned the 18 genotypes being tested in the study. Swine nutrition. A swine facility has 3 rooms containing 12 pens (i.e. 36 total pens), each with 4 pigs (note: pigs are similar across pens in a room). Within each room, 6 feeding treatments are randomly allocated to each pen (i.e. each treatment is repeated twice in each room). data.frame(`Source of variation` = c(&quot;Blocks&quot;, &quot;Treatment&quot;, &quot;Error&quot;, &quot;Total&quot;), df = c(&quot;b-1 = 3-1 = 2&quot;, &quot;t-1 = 6-1 = 5&quot;, &quot;(u-1)*b - (t-1) = (12-1)3 - (5-1) = 28&quot;, &quot;N-1 = 36 - 1 = 35&quot;)) %&gt;% knitr::kable(caption = &quot;ANOVA table for the swine example, where b is the number of blocks, t is the number of treatments, u is the number of experimental units per block, and N is the total number of observations.&quot;) Table 2.1: ANOVA table for the swine example, where b is the number of blocks, t is the number of treatments, u is the number of experimental units per block, and N is the total number of observations. Source.of.variation df Blocks b-1 = 3-1 = 2 Treatment t-1 = 6-1 = 5 Error (u-1)*b - (t-1) = (12-1)3 - (5-1) = 28 Total N-1 = 36 - 1 = 35 2.5 Homeworks Start reading: Chapter 4 in Messy Data Vol1 (Milliken &amp; Johnson) OR Chapter 2 in GLMM (Stroup, 1st ed) - 1st edition of the GLMM book, until “Complication” Schedule a meeting to discuss your project topic. "],["basic-types-of-designed-experiments-1.html", "Day 3 Basic types of designed experiments 3.1 Review 3.2 Types of designs 3.3 Building an ANOVA skeleton using design (aka topographical) and treatment elements 3.4 Linear model good old friend 3.5 Some Practice 3.6 To do", " Day 3 Basic types of designed experiments June 11th, 2025 3.1 Review Experimental unit The golden rules of designed experiments: Replication Randomization Local control (blocking) 3.2 Types of designs 3.2.1 Completely randomized design (CRD) Figure 3.1: Schematic description of an experiment with a completely randomized design 3.2.2 Randomized complete block design (RCBD) Blocks are groups of similar experimental units and are large enough to fit each treatment at least once Figure 3.2: Schematic description of an experiment with a randomized complete block design 3.3 Building an ANOVA skeleton using design (aka topographical) and treatment elements Table 3.1: Constructing the ANOVA skeleton Table 3.1: Experiment or Topographical Source df Block b-1 - Pens(Block) (u-1)*b Total N-1 Table 3.1: Treatment Source df - Treatment t-1 Parallels N-t Total N-1 Table 3.1: Combined Table Source df Block b-1 Treatment t-1 Pens(Block x Trt) (u-1)*b - (t-1) Total N-1 This way to do an ANOVA is normally considered the “What Would Fisher Do” ANOVA. Sir R.A. Fisher did plenty of his very influential work while he was working at the Rothamsted Agricultural Station! Sir R.A. Fisher 3.4 Linear model good old friend What does “linear” mean? What does “ANOVA” mean? \\[\\begin{equation} y_{ij} = \\mu + \\tau_i + \\varepsilon_{ij}, \\\\ \\varepsilon_{ij} \\sim N(0, \\sigma^2), \\end{equation}\\] OR \\[\\begin{equation} y_{i} = \\beta_0 + x_{1i}\\beta_1 +x_{2i}\\beta_2 + x_{3i}\\beta_3 + \\varepsilon_{i}, \\\\ \\varepsilon_{i} \\sim N(0, \\sigma^2), \\\\ x_{1i} = \\begin{cases} 1, &amp; \\text{if treatment A}\\\\ 0, &amp; \\text{otherwise} \\end{cases} \\\\ x_{2i} = \\begin{cases} 1, &amp; \\text{if treatment B}\\\\ 0, &amp; \\text{otherwise} \\end{cases} \\\\ x_{3i} = \\begin{cases} 1, &amp; \\text{if treatment C}\\\\ 0, &amp; \\text{otherwise} \\end{cases} \\end{equation}\\] OR \\[\\begin{equation} \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\\\ \\boldsymbol{\\varepsilon} \\sim N(\\boldsymbol{0}, \\sigma^2 \\mathbf{I}), \\\\ \\end{equation}\\] OR \\[\\begin{equation} \\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}), \\\\ \\boldsymbol{\\mu} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\end{equation}\\] 3.4.1 The most common assumptions behind most software Constant variance Independence Normality We can describe the general linear model as \\[\\begin{equation} y_{ij} = \\mu + \\tau_i + \\varepsilon_{ij}, \\end{equation}\\] \\[\\begin{equation} \\varepsilon_{ij} \\sim N(0, \\sigma^2), \\end{equation}\\] where \\(y_{ij}\\) is the \\(j\\)th observation of the \\(i\\)th treatment, \\(\\mu\\) is the overall mean, \\(\\tau_i\\) is the treatment effect of the \\(i\\)th treatment, and \\(\\varepsilon_{ij}\\) is the residual for the \\(j\\)th observation of the \\(i\\)th treatment (i.e., the difference between observed and predicted). The form used to describe the model above is called “Model equation form”. Another way of saying the same is the “Probability distribution form”, where we describe the distribution of \\(y\\) directly. \\[\\begin{equation} y_{ij} \\sim N(\\mu_{ij}, \\sigma^2), \\end{equation}\\] or \\[\\begin{equation} \\mathbf{y} \\sim N(\\mu_{ij}, \\sigma^2). \\end{equation}\\] 3.5 Some Practice Check out this R script to follow along! 3.6 To do Start reading: Chapter 4 in Messy Data Vol1 (Milliken &amp; Johnson) OR Chapter 2 in GLMM (Stroup, 1st ed) - 1st edition of the GLMM book, until “Complication” Schedule a meeting to discuss your project topic. "],["linear-models-anova-shells-applied-to-the-more-basic-experiment-designs.html", "Day 4 Linear models, ANOVA shells applied to the more basic experiment designs 4.1 Review 4.2 Linear models 4.3 Takehomes 4.4 Tomorrow", " Day 4 Linear models, ANOVA shells applied to the more basic experiment designs June 12th, 2025 4.1 Review Mindmap of the course, designed experiments, and the reason behind all these analyses. The golden rules to design experiments: Replication Randomization Local control (blocking) A good set of steps to analyze data that is handed to us: What are the treatment factors? What is the experimental unit? Is it the same as the observational unit? How were the treatments applied? (What is the blueprint of the design/underlying structure?) Building the statistical model: Deterministic component Random component (probability distribution) Estimation Method 4.2 Linear models 4.2.1 The most common model - Assumptions Common assumptions behind the default in most software: Constant variance Independence this is what is affected by the design! Normality We can describe the general linear model as \\[\\begin{equation} y_{ij} = \\mu + \\tau_i + \\varepsilon_{ij}, \\end{equation}\\] \\[\\begin{equation} \\varepsilon_{ij} \\sim N(0, \\sigma^2), \\end{equation}\\] where \\(y_{ij}\\) is the \\(j\\)th observation of the \\(i\\)th treatment, \\(\\mu\\) is the overall mean, \\(\\tau_i\\) is the treatment effect of the \\(i\\)th treatment, and \\(\\varepsilon_{ij}\\) is the residual for the \\(j\\)th observation of the \\(i\\)th treatment (i.e., the difference between observed and predicted). 4.2.2 Connection between this and your classical ANOVA table Recall that under Maximum Likelihood Estimation (MLE) and assuming a normal distribution, the estimates for MLE and Least Squares Estimation (LSE) are equivalent: \\(\\hat\\beta_{MLE} =\\hat\\beta_{LSE}\\). Least Squares means that the estimates (\\(\\hat\\beta\\)) are the ones that minimize \\(\\sum_{i=1}^ny_i-\\bar{y}\\). The SS can be divided into the ones explained by the model and the ones not explained by the model \\(SS_{total} = SS_{model} + SS_{error}\\). The SS can also be expressed dividing it into batches depending of their source of variability. The different SS are then used to get an \\(F\\) value used in the analysis of variance. The SS can also be used to estimate \\(\\sigma^2\\): \\(\\sigma^2 = \\frac{\\sum_{i=1}^n (y_i-\\hat{y_i})^2}{df_e}=\\frac{SS_{error}}{df_e}\\) \\(\\sigma^2 = \\frac{SSR}{df_e}\\) How does \\(\\sigma^2\\) affect inference? Confidence intervals: \\(CI_{95\\%} = \\hat{\\beta_j}\\pm t_{dfe, 1-\\alpha} \\cdot \\widehat{s.e.}(\\hat\\beta_j)\\) \\(\\widehat{s.e.}(\\hat\\beta_j) = \\sqrt{\\frac{\\widehat{\\sigma^2}}{s^2_x (n-1)}}\\) 4.2.3 Sorghum example The data below were generated by an experiment comparing sorghum genotypes (Omer et al., 2015). The data presented here correspond to a randomized complete block design ( design structure) that was performed to study different genotypes. Remember that blocks are assumed to be aproximately homogeneous within. Back to the code we worked on yesterday. Check out the code from class here. 4.3 Takehomes Using the sorghum study, we demonstrated that if the true underlying process was actually represented by blocks (i.e., disjoint areas in the field), library(tidyverse) library(emmeans) library(agridat) library(multcomp) # load data data(&quot;omer.sorghum&quot;) df &lt;- omer.sorghum %&gt;% filter(env == &quot;E3&quot;) options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) m_without &lt;- lm(yield ~ gen , data= df) m_with &lt;- lm(yield ~ gen + rep, data= df) 4.3.1 If you don’t include the design elements (blocks), their portion of the variance goes to the error # check residuals df with blocks m_with$df.residual ## [1] 51 summary(m_with)$sigma ## [1] 160.0855 # check df without blocks m_without$df.residual ## [1] 54 summary(m_without)$sigma ## [1] 168.9182 means_with &lt;- emmeans(m_without, ~ gen) means_without &lt;- emmeans(m_with, ~ gen) cld(means_with, method = &quot;sidak&quot;, Letters = letters) ## gen emmean SE df lower.CL upper.CL .group ## G17 285 84.5 54 116 455 a ## G06 315 84.5 54 145 484 ab ## G02 515 84.5 54 346 685 abc ## G12 539 84.5 54 370 708 abc ## G01 582 84.5 54 413 751 abc ## G14 583 84.5 54 414 752 abc ## G16 605 84.5 54 435 774 abcd ## G05 643 84.5 54 474 812 abcd ## G15 647 84.5 54 477 816 abcd ## G11 728 84.5 54 559 897 bcd ## G10 739 84.5 54 569 908 bcd ## G08 741 84.5 54 572 911 bcd ## G04 749 84.5 54 580 919 bcd ## G03 805 84.5 54 636 974 cd ## G09 813 84.5 54 644 982 cd ## G13 825 84.5 54 656 994 cd ## G07 937 84.5 54 768 1106 cd ## G18 1030 84.5 54 861 1199 d ## ## Confidence level used: 0.95 ## P value adjustment: tukey method for comparing a family of 18 estimates ## significance level used: alpha = 0.05 ## NOTE: If two or more means share the same grouping symbol, ## then we cannot show them to be different. ## But we also did not show them to be the same. cld(means_without, method = &quot;sidak&quot;, Letters = letters) ## gen emmean SE df lower.CL upper.CL .group ## G17 285 80 51 125 446 a ## G06 315 80 51 154 475 ab ## G02 515 80 51 355 676 abc ## G12 539 80 51 378 700 abcd ## G01 582 80 51 421 743 abcd ## G14 583 80 51 422 744 abcd ## G16 605 80 51 444 765 abcd ## G05 643 80 51 482 804 abcde ## G15 647 80 51 486 807 abcde ## G11 728 80 51 567 889 bcde ## G10 739 80 51 578 899 cde ## G08 741 80 51 581 902 cde ## G04 749 80 51 589 910 cde ## G03 805 80 51 644 966 cde ## G09 813 80 51 652 974 cde ## G13 825 80 51 664 986 cde ## G07 937 80 51 776 1098 de ## G18 1030 80 51 869 1191 e ## ## Results are averaged over the levels of: rep ## Confidence level used: 0.95 ## P value adjustment: tukey method for comparing a family of 18 estimates ## significance level used: alpha = 0.05 ## NOTE: If two or more means share the same grouping symbol, ## then we cannot show them to be different. ## But we also did not show them to be the same. 4.4 Tomorrow Kahoot for attendance "],["review-organizing-data-and-other-helpful-tips.html", "Day 5 Review, organizing data and other helpful tips 5.1 Kahoot! as review 5.2 Organizing data 5.3 Homework", " Day 5 Review, organizing data and other helpful tips June 13th, 2025 5.1 Kahoot! as review Code on the whiteboard 5.2 Organizing data Data Organization in Spreadsheets Excel sheet 5.3 Homework Homework is posted and due in a week "],["the-treatment-structure.html", "Day 6 The treatment structure 6.1 Announcements 6.2 Review 6.3 Treatment structure 6.4 Where is the treatment structure connected to the statistical model? 6.5 Tomorrow:", " Day 6 The treatment structure June 16th, 2025 6.1 Announcements Homework due this Friday Project proposal due this Friday Next week will be on Zoom 6.2 Review The sources of variability in an experiment: Treatment Logistics, design, topographical 6.3 Treatment structure The treatment structure in an experiment is directly connected to the research question, i.e. what the researchers want to study. 6.3.1 Types of treatment structures One-way treatment structure: a set of \\(t\\) treatments or populations where there is no assumed structure among the treatments. Figure 6.1: Schematic description of an experiment with a one-way treatment structure Two-way treatment structure: a set of treatments constructed by combining the levels or possibilities of two factors. Two-way factorial treatment structure. Figure 6.2: Schematic description of an experiment with a two-way treatment structure Factorial arrangement treatment structure: a set of treatments constructed by combining the levels or possibilities of two or more different factors. Figure 6.3: Schematic description of an experiment with a three-way treatment structure Fractional factorial arrangement treatment structure: only a part, or fraction, of the possible treatment combinations in a factorial arrangement treatment structure. Figure 6.4: Schematic description of an experiment with a two-way fractional treatment structure Factorial arrangement treatment structure with one or more controls. Figure 6.5: Schematic description of an experiment with a two-way treatment structure with two controls Examples on the whiteboard 6.4 Where is the treatment structure connected to the statistical model? Remember: the general advice is to divide the sources of variation into treatment and design (i.e., topographical). Normally, the treatments affect the expected value and the experiment design has created groups of data that were generated together. Those groups are groups of similar data and that’s why we say that they are correlated. For now, we’ll leave said correlation in the residual variance-covariance matrix \\(\\mathbf{R}\\). One-way treatment structure \\[\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\mathbf{R}), \\\\ \\mu_{j} = \\mu + \\tau_j \\] Two-way treatment structure \\[\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\mathbf{R}), \\\\ \\mu_{jk} = \\mu + \\tau_j + \\rho_k +(\\tau \\rho)_{jk}\\] Factorial treatment structure \\[\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\mathbf{R}), \\\\ \\mu_{jkl} = \\mu + \\tau_j + \\rho_k + \\gamma_l +(\\tau \\rho)_{jk}+(\\tau \\gamma)_{jl} + ( \\rho \\gamma)_{kl} + (\\tau \\rho \\gamma)_{jkl}\\] Fractional factorial treatment structure, factorial treatment structure with controls Some version of: \\[\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\mathbf{R}), \\\\ \\mu_{jk} = \\mu + \\tau_j + \\rho_k +(\\tau \\rho)_{jk}\\] 6.5 Tomorrow: All this is in Chapter 4 in Milliken and Johnson. "],["what-you-ask-of-a-designed-experiment.html", "Day 7 What you ask of a designed experiment 7.1 Announcements 7.2 Review 7.3 ANOVA 7.4 Setting the stage: Estimated marginal means aka least squares means 7.5 Tomorrow", " Day 7 What you ask of a designed experiment June 17th, 2025 7.1 Announcements Homework due this Friday Project proposal due this Friday Next week will be on Zoom 7.2 Review Pre-selected treatments are usually assumed to affect the expected value. Today we’ll focus on those means of interest. Figure 7.1: Mindmap of the analysis of a designed experiment, from the inception to the end conclusions 7.3 ANOVA Table 7.1: Treatment ANOVA for a one-way treatment structure Source df Treatment t-1 Parallels N-t Total N-1 Table 7.2: Treatment ANOVA for a two-way treatment structure Source df Factor A a-1 Factor B b-1 A x B (a-1)(b-1) Parallels N-(ab) Total N-1 Table 7.3: Treatment ANOVA for a three-way factorial treatment structure Source df Factor A a-1 Factor B b-1 Factor C c-1 A x B (a-1)(b-1) A x C (a-1)(c-1) B x C (b-1)(c-1) A x B x C (a-1)(b-1)(c-1) Parallels N-(abc) Total N-1 7.3.1 In case you were wondering: ANOVA and types of sums of squares Type I SS: ordered Type II SS: conditional on main effects Type III SS: conditional on all effects From SS to test F value, to hypothesis test: \\(F = \\frac{SS_{t}/df_t}{SS_{e}/df_e}\\) 7.4 Setting the stage: Estimated marginal means aka least squares means Sometimes, model coefficients or effects are hard to interpret. In designed experiments, we often use the estimated marginal means or least square means to provide a more interpretable result. Estimated marginal means are the expected mean for a given level of a factor, averaging over the other factors in the model. In R, the estimated marginal means are famously handled with the emmeans package. 7.4.1 Example: Download the R script to follow along! More about estimated marginal means: emmeans website [link] 7.4.2 Discussion What is the risk of making inference over a single treatment factor when the estimated interaction seemed to be relevant to explain variability in the data? Where do the degrees of freedom come from? 7.5 Tomorrow More hands-on practice. Your moment to ask the questions about least square means you’ve always wanted to ask. "],["applied-examples.html", "Day 8 Applied examples 8.1 Announcements 8.2 Applied example for today 8.3 Tomorrow", " Day 8 Applied examples June 18th, 2025 8.1 Announcements Homework due this Friday Project proposal due this Friday Next week will be on Zoom Your moment to ask the questions about mean estimation/multiple comparison you’ve always wanted to know but were too afraid to ask. [Note: I will try to include these in class, but many might be covered in STAT 870] From yesterday: ANOVA SS demo [R script] url &lt;- &quot;https://raw.githubusercontent.com/stat720/summer2025/refs/heads/main/data/blood_study_pigs.csv&quot; df_pigs &lt;- read.csv(url) ## Type I m_intercept.only &lt;- lm(Serum_haptoglobin_mg.dL ~ 1, data = df_pigs) m_Trt.only &lt;- lm(Serum_haptoglobin_mg.dL ~ Trt, data = df_pigs) m_Trt.Day &lt;- lm(Serum_haptoglobin_mg.dL ~ Trt+factor(Day), data = df_pigs) m_Trt.Day.TxD &lt;- lm(Serum_haptoglobin_mg.dL ~ Trt*factor(Day), data = df_pigs) sum(m_intercept.only$residuals^2) ## [1] 212318.2 sum(m_intercept.only$residuals^2) - sum(m_Trt.only$residuals^2) ## [1] 81720.2 sum(m_Trt.only$residuals^2) - sum(m_Trt.Day$residuals^2) ## [1] 69494.52 sum(m_Trt.Day$residuals^2) - sum(m_Trt.Day.TxD$residuals^2) ## [1] 24352.12 sum(m_Trt.Day.TxD$residuals^2) ## [1] 36751.37 anova(m_Trt.Day.TxD) ## Analysis of Variance Table ## ## Response: Serum_haptoglobin_mg.dL ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Trt 5 81720 16344 80.049 &lt; 2.2e-16 *** ## factor(Day) 1 69495 69495 340.369 &lt; 2.2e-16 *** ## Trt:factor(Day) 5 24352 4870 23.854 &lt; 2.2e-16 *** ## Residuals 180 36751 204 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Type II m_intercept.only &lt;- lm(Serum_haptoglobin_mg.dL ~ 1, data = df_pigs) m_Day.only &lt;- lm(Serum_haptoglobin_mg.dL ~ factor(Day), data = df_pigs) m_Trt.only &lt;- lm(Serum_haptoglobin_mg.dL ~ Trt , data = df_pigs) m_Trt.Day &lt;- lm(Serum_haptoglobin_mg.dL ~ Trt + factor(Day), data = df_pigs) m_Trt.Day.TxD &lt;- lm(Serum_haptoglobin_mg.dL ~ Trt*factor(Day), data = df_pigs) sum(m_intercept.only$residuals^2) ## [1] 212318.2 sum(m_Day.only$residuals^2) - sum(m_Trt.Day$residuals^2) ## [1] 81720.2 sum(m_Trt.only$residuals^2) - sum(m_Trt.Day$residuals^2) ## [1] 69494.52 sum(m_Trt.Day$residuals^2) - sum(m_Trt.Day.TxD$residuals^2) ## [1] 24352.12 sum(m_Trt.Day.TxD$residuals^2) ## [1] 36751.37 car::Anova(m_Trt.Day.TxD, type = 2) ## Anova Table (Type II tests) ## ## Response: Serum_haptoglobin_mg.dL ## Sum Sq Df F value Pr(&gt;F) ## Trt 81720 5 80.049 &lt; 2.2e-16 *** ## factor(Day) 69495 1 340.369 &lt; 2.2e-16 *** ## Trt:factor(Day) 24352 5 23.854 &lt; 2.2e-16 *** ## Residuals 36751 180 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 8.2 Applied example for today R script 8.3 Tomorrow Holiday. Kahoot! on Friday. "],["review-5.html", "Day 9 Review 9.1 Announcements 9.2 Next week", " Day 9 Review June 20th, 2025 9.1 Announcements Homework due today New Homework posted today, due Wednesday, July 2nd. Project proposal due today Next week will be on Zoom Your moment to ask the questions about mean estimation/multiple comparison you’ve always wanted to know but were too afraid to ask. [Note: I will try to include these in class, but many might be covered in STAT 870] 9.2 Next week Zoom classes – we will use the link for the office hours "],["applied-linear-mixed-models-i.html", "Day 10 Applied Linear Mixed Models I 10.1 Announcements 10.2 Applied Linear Mixed Models 10.3 Review: Designs 10.4 Tomorrow", " Day 10 Applied Linear Mixed Models I June 23th, 2025 10.1 Announcements Homeworks review 10.2 Applied Linear Mixed Models Today and tomorrow, we’ll focus on the models behind most models for analyzing designed experiments. 10.2.1 Review: random variables Data are typically considered a random variable (e.g., \\(y\\sim N(\\mu, \\sigma^2)\\). Random variables are usually described with their properties like the expected value and variance. The expected value and variance are the first and second central moments of a distribution, respectively. Regardless of the distribution of a random variable \\(y\\), we could calculate its expected value \\(E(y)\\) and variance \\(Var(y)\\). The expected value measures the average outcome of \\(y\\). The variance measures the dispersion of \\(y\\), i.e. how far the possible outcomes are spread out from their average. 10.2.1.1 Demo: normally distributed random variable 10.2.1.2 Review: variance-covariance The variance of a random variable can be conceived as the level of dispersion in a variable. The covariance between two random variables means how the two random variables behave relative to each other. Essentially, it quantifies the relationship between their joint variability. For example, the covariance between two variables \\(y_1\\) and \\(y_2\\) will make Note that the variance of a random variable is the covariance of a random variable with itself. Consider two variables \\(y_1\\) and \\(y_2\\) each with a variance of 1 and a covariance of 0.6. Using the notation we’ve been using in class, that can be written as \\[\\mathbf{y} \\sim N (\\boldsymbol{\\mu}, \\mathbf{V}),\\] where \\(\\mathbf{y}\\) is the vector containing the variables \\(y_1\\) and \\(y_2\\), \\(\\mathbf{y} \\equiv [y_1, y_2]&#39;\\), \\(\\boldsymbol{\\mu}\\) is the mean vector, \\(\\boldsymbol{\\mu} \\equiv [0,0]&#39;\\), and \\(\\mathbf{V}\\) is the variance-covariance matrix. We can say that \\(\\mathbf{V} = \\begin{bmatrix} 1 &amp; 0.6 \\\\ 0.6 &amp; 1\\end{bmatrix}\\). set.seed(43) # some covariance mgcv::rmvn(1, c(0,0), matrix(c(1, .6, .6, 1), nrow=2, ncol=2)) ## [1] -0.03751376 -1.28219179 mgcv::rmvn(1, c(0,0), matrix(c(1, .6, .6, 1), nrow=2, ncol=2)) ## [1] -0.48596752 0.08056847 mgcv::rmvn(1, c(0,0), matrix(c(1, .6, .6, 1), nrow=2, ncol=2)) ## [1] -0.9040981 -0.7644051 mgcv::rmvn(1, c(0,0), matrix(c(1, .6, .6, 1), nrow=2, ncol=2)) ## [1] 0.3864344 0.1835374 # no covariance mgcv::rmvn(1, c(0,0), matrix(c(1, 0, 0, 1), nrow=2, ncol=2)) ## [1] -0.6861798 -1.9061368 mgcv::rmvn(1, c(0,0), matrix(c(1, 0, 0, 1), nrow=2, ncol=2)) ## [1] 1.8037598 -0.9668729 mgcv::rmvn(1, c(0,0), matrix(c(1, 0, 0, 1), nrow=2, ncol=2)) ## [1] -0.3531183 1.1069375 mgcv::rmvn(1, c(0,0), matrix(c(1, 0, 0, 1), nrow=2, ncol=2)) ## [1] 0.5663244 2.0643087 10.3 Review: Designs When we model data generated by designed experiments, we typically put the information from the treatment as affecting the mean (\\(\\mu\\)), while the elements from the design (topographical, design, or the logistics of the experiment) indicate which observations are independent and which ones are not. Completely randomized designs: independent EUs Randomized complete block designs: groups of similar EUs 10.4 Tomorrow Zoom classes – we will use the link for the office hours "],["applied-linear-mixed-models-ii.html", "Day 11 Applied Linear Mixed Models II 11.1 Review: Designs 11.2 Tomorrow", " Day 11 Applied Linear Mixed Models II June 24th, 2025 11.1 Review: Designs When we model data generated by designed experiments, we typically put the information from the treatment as affecting the mean (\\(\\mu\\)), while the elements from the design (topographical, design, or the logistics of the experiment) indicate which observations are independent and which ones are not. Completely randomized designs: independent EUs that allow us to make the assumption of independence. Randomized complete block designs: groups of similar EUs: the assumption of independence does not stay the same anymore, but we can say that observations from a same block are all related to each other. Other types of designs: split-plot and strip-plot 11.1.1 Let’s look at the anova table Table 11.1: Treatment ANOVA for a two-way treatment structure Source df Factor A a-1 Factor B b-1 A x B (a-1)(b-1) Parallels N-(ab) Total N-1 11.2 Tomorrow Zoom classes – we will use the link for the office hours "],["practice-hierarchical-multilevel-designs.html", "Day 12 Practice: Hierarchical (Multilevel) Designs 12.1 Review: Hierarchical Designs 12.2 R demo 12.3 Tomorrow", " Day 12 Practice: Hierarchical (Multilevel) Designs June 25th, 2025 12.1 Review: Hierarchical Designs Remember the definition of experimental unit? The smallest unit to which a treatment is independently applied. Sometimes we find that there are different sizes of experimental units. In such cases, it is important to identify the different experimental units and the randomization scheme. We may be in front of a multilevel design. Figure 12.1: Schematic description of a field experiment with a split-plot design Figure 12.2: Schematic description of a swine experiment with a split-plot design Sometimes, these differences in the sizes of EUs are not that easy to notice. More details in Analysis of Messy Data - Ch5. 12.1.1 More pictures: Example 1 - RCBD Example 2 - RCBD Example 3 12.2 R demo Follow along with this R script! 12.3 Tomorrow Zoom classes – we will use the link for the office hours "],["more-practice-hierarchical-multilevel-designs.html", "Day 13 More practice: Hierarchical (Multilevel) Designs 13.1 Review: Hierarchical Designs 13.2 Building the ANOVA skeleton using design (aka topographical) and treatment elements 13.3 R demo 13.4 Tomorrow", " Day 13 More practice: Hierarchical (Multilevel) Designs June 26th, 2025 13.1 Review: Hierarchical Designs Remember the definition of experimental unit? The smallest unit to which a treatment is independently applied. Sometimes we find that there are different sizes of experimental units. In such cases, it is important to identify the different experimental units and the randomization scheme. We may be in front of a multilevel design. Figure 13.1: Schematic description of a field experiment with a split-plot design Figure 13.2: Schematic description of a swine experiment with a split-plot design Sometimes, these differences in the sizes of EUs are not that easy to notice. More details in Analysis of Messy Data - Ch5. 13.1.1 Remember our example: Rows and beds (aka columns) probably looked somewhat like this: library(tidyverse) library(agridat) library(ggpubr) data(&quot;durban.splitplot&quot;) df &lt;- durban.splitplot theme_set(theme_minimal()) p_blocks &lt;- df %&gt;% ggplot(aes(bed, row))+ geom_tile(aes(fill = block))+ geom_tile(color = &quot;black&quot;, fill=NA)+ coord_fixed() p_wholeplot &lt;- df %&gt;% ggplot(aes(bed, row))+ geom_tile(aes(fill = fung))+ geom_tile(color = &quot;black&quot;, fill=NA)+ coord_fixed() p_splitplot &lt;- df %&gt;% ggplot(aes(bed, row))+ geom_tile(aes(fill = gen), show.legend= F)+ geom_tile(color = &quot;black&quot;, fill=NA)+ coord_fixed() ggarrange(p_blocks, p_wholeplot, p_splitplot, ncol = 1, nrow = 3) 13.2 Building the ANOVA skeleton using design (aka topographical) and treatment elements Table 13.1: Constructing the ANOVA skeleton Table 13.1: Experiment or Topographical Source df Block b-1 - Fungicide(Block) (f-1)*b - - Gen(Fung x Block) (g-1)fb Total N-1 Table 13.1: Treatment Source df - - Fungicide f-1 - Genotype g-1 Fung x Gen (f-1)(g-1) Parallels N-(f*g) Total N-1 Table 13.1: Combined Table Source df Block b-1 Fungicide t-1 Fungicide(Block) (f-1)*b - (t-1) Genotype g-1 Fung x Gen (f-1)(g-1) Pens(Block x Trt) error (g-1)* f * b - (g-1 + (f-1)(g-1)) Total N-1 13.3 R demo Follow along with this R script! 13.4 Tomorrow Zoom classes – we will use the link for the office hours "],["review-6.html", "Day 14 Review", " Day 14 Review June 30th, 2025 HW 3 is posted and due next Tuesday. Kahoot! "],["review-rcbd-and-split-plot.html", "Day 15 Review: RCBD and split-plot 15.1 Background 15.2 Study / Research question 15.3 Tomorrow:", " Day 15 Review: RCBD and split-plot July 1st, 2025 Today, tomorrow and Wednesday we’ll do a hands-on demo on designing an experiment. 15.1 Background Just like banana bread, banana muffins are different to other types of muffins because their dough includes bananas, which sometimes makes it challenging to get a fluffy crumb. If you don’t manage your oven temperature and amount of bananas properly, you might end up with a dough that is too heavy, does not rise properly, and ends up like the picture below. We’ll start with our go-to banana recipe and fine-tune the best baking temperature to get fluffy muffins. Check out the recipe here. Banana bread goes wrong 15.1.1 Baseline recipe for banana muffins: Dry ingredients: 1 1/2 cups all-purpose flour (7 1/2 ounces) 1 1/2 teaspoons baking powder 1/4 teaspoon baking soda 1/2 teaspoon salt 1/4 teaspoon ground cinnamon 1/4 teaspoon ground nutmeg 2/3 cup finely chopped pecans, toasted Wet ingredients: 1/2 cup light brown sugar (3 3/4 ounces) 1/4 cup granulated sugar (2 ounces) 2 large eggs 1 stick unsalted butter, browned and cooled (4 ounces) 1 1/2 cups mashed bananas, from very ripe bananas that have been peeled and mashed well with a fork (12 3/4 ounces) 2 tablespoons full-fat sour cream 1 teaspoon pure vanilla extract 15.2 Study / Research question What is the best temperature to bake the muffins? Let’s assume that we prefer higher muffins: what temperature will produce higher muffins? 250 °F 400 °F 550 °F 15.2.1 Proposed design experiment: RCBD Assuming we spread out all the work into multiple days, we can assume cooking day as the block. Then, the model to describe variations in the muffin height can be written as \\[y_{ik} = \\mu + b_k + T_i + \\varepsilon_{ik},\\] \\[b_k \\sim N(0, \\sigma_b^2), \\\\ \\varepsilon_{ik} \\sim N(0, \\sigma_{\\varepsilon}^2),\\] where \\(y_{ik}\\) is the observed height for the muffin baked under the \\(i\\)th treatment on day \\(k\\), \\(\\mu\\) is the overall muffin height, \\(T_i\\) is the effect of the \\(i\\)th temperature on muffin height, \\(b_k\\) is the day (block) effect, that arises from a normal distribution with variance \\(\\sigma^2_b\\), and \\(\\varepsilon_{ik}\\) is the residual for the muffin baked under the \\(i\\)th treatment on day \\(k\\). The final ANOVA table for this RCBD design is: Table 15.1: Treatment ANOVA for a one-way treatment structure in an RCBD Source df Day b-1 = 3-1 = 2 Temperature t-1 = 3-1 = 2 Error(oven) (t-1)b - (t-1) = 2*3 - 2 = 4 Total N-1 = 8 15.2.2 Treatment means and confidence intervals The treatment mean for the \\(i\\)th temperature is \\(\\mu_i = \\mu + T_i\\). That mean won’t change under different design structures. What might change is the confidence interval around that mean. [For more information, see Chapter 24 in Milliken and Johnson’s Analysis of Messy Data]. First, recall the formula for a CI: \\(\\theta \\pm t_{df, \\frac{\\alpha}{2}} \\cdot se(\\hat{\\theta})\\) #get test t qt(p = .975, df = 4) # df are df error(oven) ## [1] 2.776445 # standard error sqrt((2*sigma(m)^2) / 3) For example, the CI for the differences between means at 400F - 300F \\(\\mu_{400} - \\mu_{300}\\) is \\((\\widehat{\\mu_{400} - \\mu_{300}}) \\pm 2.78 \\cdot se(\\widehat{\\mu_{400} - \\mu_{300}})\\) \\(se(\\widehat{\\mu_{400} - \\mu_{300}}) = \\sqrt{\\frac{2 \\sigma^2_{\\varepsilon}}{r}}\\) \\[\\mu_i \\pm 2.78 \\cdot \\sqrt{\\frac{2 \\sigma^2_{\\varepsilon}}{r}}\\] 15.3 Tomorrow: What if we want to include more bananas in the recipe? Expand this design including a 3x2 treatment structure in a split-plot design. "],["review-applied-split-plot.html", "Day 16 Review: Applied split-plot 16.1 Announcements 16.2 Background 16.3 What is the best design? 16.4 Treatment means and confidence intervals for the split-plot design 16.5 Tomorrow:", " Day 16 Review: Applied split-plot July 2nd, 2025 16.1 Announcements Planning to miss &gt;2 classes in July? survey Watch last week’s classes (especially days 3+4) Homework 2 due today. Homework 3 is posted and due next Friday (July 11). 16.2 Background Yesterday, we designed an experiment with an RCBD to figure out the best temperature to bake the muffins. Check out the original recipe here. Yesterday we agreed on the following model: \\[y_{ik} = \\mu + T_i + b_k + \\varepsilon_{ik},\\] \\[b_k \\sim N(0, \\sigma_b^2), \\\\ \\varepsilon_{ik} \\sim N(0, \\sigma_{\\varepsilon}^2),\\] and design: Table 16.1: Table 16.1: Design Sources of Variability Source df Day (block) r-1 = 3-1 = 2 - Error(oven) (t-1)r = (3-1)*3 = 6 Total N-1 = 8 Table 16.1: Treatment Sources of Variability Source df - Temperature t-1 = 3-1 = 2 Parallels N-t = 9-3 = 6 Total N-1 = 8 Table 16.1: Combined Table of the Sources of Variability Source df Day r-1 = 3-1 = 2 Temperature t-1 = 3-1 = 2 Error(oven) (t-1)r - (t-1) = 2*3 - 2 = 4 Total N-1 = 8 Dry ingredients: 1 1/2 cups all-purpose flour (7 1/2 ounces) 1 1/2 teaspoons baking powder 1/4 teaspoon baking soda 1/2 teaspoon salt 1/4 teaspoon ground cinnamon 1/4 teaspoon ground nutmeg 2/3 cup finely chopped pecans, toasted Wet ingredients: 1/2 cup light brown sugar (3 3/4 ounces) 1/4 cup granulated sugar (2 ounces) 2 large eggs 1 stick unsalted butter, browned and cooled (4 ounces) 1 1/2 cups mashed bananas, from very ripe bananas that have been peeled and mashed well with a fork (12 3/4 ounces or 361 gr.) 2 tablespoons full-fat sour cream 1 teaspoon pure vanilla extract 16.2.1 Research question What is the best temperature to bake the muffins? 300 °F 400 °F 500 °F How much banana? 1 1/2 cups (12.75 oz, or 361 gr.) 2 cups (17 oz, or 482 gr.) 16.3 What is the best design? 16.3.1 Option A: another RCBD Corresponding model: \\[y_{ik} = \\mu + T_i + B_j + (TB)_{ij} + b_k + \\varepsilon_{ik},\\] \\[b_k \\sim N(0, \\sigma_b^2), \\\\ \\varepsilon_{ik} \\sim N(0, \\sigma_{\\varepsilon}^2).\\] Table 16.2: Table 16.2: Design Sources of Variability Source df Day (block) r-1 = 3-1 = 2 - - - Error(oven) (tb-1)r = (6-1)3 = 15 Total N-1 = 17 Table 16.2: Treatment Sources of Variability Source df - Temperature t-1 = 3-1 = 2 Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Parallels N-tb = 18 - (3*2) = 12 Total N-1 = 17 Table 16.2: Combined Table of the Sources of Variability Source df Day r-1 = 3-1 = 2 Temperature t-1 = 3-1 = 2 Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Error(oven) (tb-1)r - (t-1) - (b-1) - (t-1)(b-1) = 15 - 2 -1 -2 = 10 Total N-1 = 17 16.3.2 Option B: split-plot design In the model: Cooking days are blocks. Oven are “mini-blocks” for banana recipe. Statistical model: \\[y_{ik} = \\mu + T_i + B_j + (TB)_{ij} + b_k + w_{i(k)} + \\varepsilon_{ik},\\] \\[b_k \\sim N(0, \\sigma_b^2), \\\\ w_{i(k)} \\sim N(0, \\sigma^2_w) , \\\\ \\varepsilon_{ik} \\sim N(0, \\sigma_{\\varepsilon}^2).\\] ANOVA table for this split-plot design Table 16.3: Table 16.3: Design or Topographical Sources of Variability Source df Day (block) r-1 = 3-1 = 2 - Error(oven) (t-1)r = (3-1)3 = 6 - - Error(oven x day) (b-1)* t * r = (2-1) * 3 * 3 = 9 Total N-1 = 17 Table 16.3: Treatment Sources of Variability Source df - Temperature t-1 = 3-1 = 2 - Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Parallels N-tb = 18 - (3*2) = 12 Total N-1 = 17 Table 16.3: Combined Table of the Sources of Variability Source df Day r-1 = 3-1 = 2 Temperature t-1 = 3-1 = 2 Error(oven) (t-1)*r - (t-1)= 6 -2 = 4 Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Error(oven x day) (b-1)* t * r - (b-1) - (t-1)*(b-1) = 9 - 1 -2 = 6 Total N-1 = 17 16.4 Treatment means and confidence intervals for the split-plot design The treatment mean for the \\(i\\)th temperature and \\(j\\)th banana level is \\(\\mu_{ij} = \\mu + T_i + B_j +(TB)_{ij}\\). That mean won’t change under different design structures. What may change is the confidence interval around the mean difference. First, recall the formula for a CI: \\(\\theta \\pm t_{df, \\frac{\\alpha}{2}} \\cdot se(\\hat{\\theta})\\) #get test t qt(p = .975, df = 4) # df are df error(oven) ## [1] 2.776445 For example, the CI for the differences between means for 300F and 400F \\(\\mu_{1 \\cdot} - \\mu_{2 \\cdot}\\) is \\((\\mu_{1 \\cdot} - \\mu_{2 \\cdot}) \\pm 2.78 \\cdot se(\\widehat{\\mu_{1 \\cdot} - \\mu_{2 \\cdot}})\\) \\(se(\\widehat{\\mu_{1 \\cdot} - \\mu_{2 \\cdot}}) = \\sqrt{\\frac{2 (\\sigma^2_{\\varepsilon} + b \\cdot \\sigma^2_w)}{b \\cdot r}}\\) \\[\\mu_i \\pm 2.78 \\cdot \\sqrt{\\frac{2 (\\sigma^2_{\\varepsilon} + b \\cdot \\sigma^2_w)}{b \\cdot r}}\\] #get test t qt(p = .975, df = 6) # df are df error(oven) ## [1] 2.446912 The CI for the differences between means for normal and high banana \\(\\mu_{\\cdot 1} - \\mu_{\\cdot 2}\\) is \\((\\mu_{\\cdot 1} - \\mu_{\\cdot 2}) \\pm 2.44 \\cdot se(\\widehat{\\mu_{\\cdot 1} - \\mu_{\\cdot 2}})\\) \\(se(\\widehat{\\mu_{\\cdot 1} - \\mu_{\\cdot 2}}) = \\sqrt{\\frac{2 \\sigma^2_{\\varepsilon}}{t \\cdot r}}\\) \\[\\mu_i \\pm 2.44 \\cdot \\sqrt{\\frac{2 \\sigma^2_{\\varepsilon}}{r}}\\] 16.5 Tomorrow: Measure heights of the muffins and analyze the data! "],["analyzing-data-from-a-split-plot-design.html", "Day 17 Analyzing data from a split-plot design 17.1 Announcements 17.2 Background 17.3 Analyzing the data 17.4 Treatment means and confidence intervals for the split-plot design 17.5 Tomorrow:", " Day 17 Analyzing data from a split-plot design July 3rd, 2025 17.1 Announcements Planning to miss &gt;2 classes in July? survey Watch last week’s classes (especially days 3+4) Homework 3 is posted and due next Friday (July 11). 17.2 Background We designed an experiment a split-plot design to figure out the best temperature and recipe to bake the muffins. Check out the original recipe here. An appropriate model to describe the data is: \\[y_{ijk} = \\mu + T_i + R_j + (TR)_{ij} + b_k + w_{i(k)} + \\varepsilon_{ijk},\\] \\[b_k \\sim N(0, \\sigma_b^2), \\\\w_{i(k)} \\sim N(0, \\sigma^2_w), \\\\ \\varepsilon_{ijk} \\sim N(0, \\sigma_{\\varepsilon}^2).\\] 17.2.1 Research question What is the best temperature to bake the muffins? 250 °F 400 °F 500 °F How much banana? 1 1/2 cups (12.75 oz, or 361 gr.) 2 cups (17 oz, or 482 gr.) Table 17.1: ANOVA without subsampling Table 17.1: Design or Topographical Sources of Variability Source df Day (block) r-1 = 3-1 = 2 - Error(oven) (t-1)r = (3-1)3 = 6 - - Error(oven x day) (b-1)* t * r = (2-1) * 3 * 3 = 9 Total N-1 = 17 Table 17.1: Treatment Sources of Variability Source df - Temperature t-1 = 3-1 = 2 - Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Parallels N-tb = 18 - (3*2) = 12 Total N-1 = 17 Table 17.1: Combined Table of the Sources of Variability Source df Day r-1 = 3-1 = 2 Temperature t-1 = 3-1 = 2 Error(oven) (t-1)*r - (t-1)= 6 -2 = 4 Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Error(oven x day) (b-1)* t * r - (b-1) - (t-1)*(b-1) = 9 - 1 -2 = 6 Total N-1 = 17 Table 17.2: ANOVA with subsampling Table 17.2: Design or Topographical Sources of Variability Source df Day (block) r-1 = 3-1 = 2 - Error(oven) (t-1)r = (3-1)3 = 6 - - Error(oven x day) (b-1)* t * r = (2-1) * 3 * 3 = 9 Error(recipe x oven x day) (m-1) * b * t * r = (3-1) * 2 * 3 * 3 = 36 Total N-1 = 53 Table 17.2: Treatment Sources of Variability Source df - Temperature t-1 = 3-1 = 2 - Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 - Parallels N-tb = 54 - (3*2) = 48 Total N-1 = 17 Table 17.2: Combined Table of the Sources of Variability Source df Day r-1 = 3-1 = 2 Temperature t-1 = 3-1 = 2 Error(oven) (t-1)*r - (t-1)= 6 -2 = 4 Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Error(oven x day) (b-1)* t * r - (b-1) - (t-1)*(b-1) = 9 - 1 -2 = 6 Error(recipe x oven x day) (m-1) * b * t * r - 0 = 36 Total N-1 = 53 Figure 17.1: Muffin experiment 17.3 Analyzing the data Get code here. 17.4 Treatment means and confidence intervals for the split-plot design The treatment mean for the \\(i\\)th temperature and \\(j\\)th banana level is \\(\\mu_{ij} = \\mu + T_i + B_j +(TB)_{ij}\\). That mean won’t change under different design structures. What may change is the confidence interval around the mean difference. First, recall the formula for a CI: \\(\\theta \\pm t_{df, \\frac{\\alpha}{2}} \\cdot se(\\hat{\\theta})\\) #get test t qt(p = .975, df = 4) # df are df error(oven) ## [1] 2.776445 For example, the CI for the differences between means for 300F and 400F \\(\\mu_{1 \\cdot} - \\mu_{2 \\cdot}\\) is \\((\\mu_{1 \\cdot} - \\mu_{2 \\cdot}) \\pm 2.78 \\cdot se(\\widehat{\\mu_{1 \\cdot} - \\mu_{2 \\cdot}})\\) \\(se(\\widehat{\\mu_{1 \\cdot} - \\mu_{2 \\cdot}}) = \\sqrt{\\frac{2 (\\sigma^2_{\\varepsilon} + b \\cdot \\sigma^2_w)}{b \\cdot r}}\\) \\[\\mu_i \\pm 2.78 \\cdot \\sqrt{\\frac{2 (\\sigma^2_{\\varepsilon} + b \\cdot \\sigma^2_w)}{b \\cdot r}}\\] #get test t qt(p = .975, df = 6) # df are df error(oven) ## [1] 2.446912 The CI for the differences between means for normal and high banana \\(\\mu_{\\cdot 1} - \\mu_{\\cdot 2}\\) is \\((\\mu_{\\cdot 1} - \\mu_{\\cdot 2}) \\pm 2.44 \\cdot se(\\widehat{\\mu_{\\cdot 1} - \\mu_{\\cdot 2}})\\) \\(se(\\widehat{\\mu_{\\cdot 1} - \\mu_{\\cdot 2}}) = \\sqrt{\\frac{2 \\sigma^2_{\\varepsilon}}{t \\cdot r}}\\) \\[\\mu_i \\pm 2.44 \\cdot \\sqrt{\\frac{2 \\sigma^2_{\\varepsilon}}{r}}\\] 17.5 Tomorrow: Measure heights of the muffins and analyze the data! "],["analysis-and-inference-for-a-split-plot-design.html", "Day 18 Analysis and inference for a split-plot design 18.1 Announcements 18.2 Review of our experiment 18.3 ANOVA tables 18.4 Applied analysis in R", " Day 18 Analysis and inference for a split-plot design July 7th, 2025 18.1 Announcements Homework 3 is due this Friday (July 11). Homework 2 grades will be posted this evening. Semester project: An example is posted. Wednesday July 23: send project for peer review. Schedule for somewhere between July 21-August 1 (anywhere between 8am-5pm): 15 min presentation + 15 min Q&amp;A. Submit final version of your report by August 1. Include ANOVA, stat model, mock R code, discussion of strengths and weaknesses of the experiment design. 18.2 Review of our experiment Research question: can we include more banana than the original recipe? Will the optimum temperature change depending on that recipe? Treatment structure: 3 \\(\\times\\) 2 factorial, with 3 levels for temperature (250, 400, 500) and 2 levels for recipe (Control, Extra Banana) Design structure: split-plot in an CRD, with temperature in the whole plot. Figure 18.1: Muffin experiment With that treatment structure, the statistical model will always begin with \\(y_{ijk} = \\mu + T_i + R_j +(TR)_{ij}\\), followed by the random effects. The random effects will depend on the design structure. 18.3 ANOVA tables 18.3.1 Split-plot in a CRD Table 18.1: ANOVA without subsampling Table 18.1: Design or Topographical Sources of Variability Source df - Error(oven) t*r-1 = 9-1 = 8 - - Error(oven) (b-1)* t * r = (2-1) * 3 * 3 = 9 Total N-1 = 17 Table 18.1: Treatment Sources of Variability Source df Temperature t-1 = 3-1 = 2 - Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Parallels N-tb = 18 - (3*2) = 12 Total N-1 = 17 Table 18.1: Combined Table of the Sources of Variability Source df Temperature t-1 = 3-1 = 2 Error(oven) t*r-1 - (t-1)= 8 - 2 = 6 Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Error(oven x day) (b-1)* t * r - (b-1) - (t-1)*(b-1) = 9 - 1 -2 = 6 Total N-1 = 17 18.3.2 Other possible designs 18.3.2.1 Split-plot in an RCBD Table 18.2: ANOVA without subsampling Table 18.2: Design or Topographical Sources of Variability Source df Day (block) r-1 = 3-1 = 2 - Error(oven) (t-1)r = (3-1)3 = 6 - - Error(oven x day) (b-1)* t * r = (2-1) * 3 * 3 = 9 Total N-1 = 17 Table 18.2: Treatment Sources of Variability Source df - Temperature t-1 = 3-1 = 2 - Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Parallels N-tb = 18 - (3*2) = 12 Total N-1 = 17 Table 18.2: Combined Table of the Sources of Variability Source df Day r-1 = 3-1 = 2 Temperature t-1 = 3-1 = 2 Error(oven) (t-1)*r - (t-1)= 6 -2 = 4 Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Error(oven x day) (b-1)* t * r - (b-1) - (t-1)*(b-1) = 9 - 1 -2 = 6 Total N-1 = 17 18.3.2.2 RCBD Table 18.3: ANOVA with subsampling Table 18.3: Design or Topographical Sources of Variability Source df Day (block) r-1 = 3-1 = 2 - Error(oven) (t-1)r = (3-1)3 = 6 - - Error(oven x day) (b-1)* t * r = (2-1) * 3 * 3 = 9 Error(recipe x oven x day) (m-1) * b * t * r = (3-1) * 2 * 3 * 3 = 36 Total N-1 = 53 Table 18.3: Treatment Sources of Variability Source df - Temperature t-1 = 3-1 = 2 - Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 - Parallels N-tb = 54 - (3*2) = 48 Total N-1 = 17 Table 18.3: Combined Table of the Sources of Variability Source df Day r-1 = 3-1 = 2 Temperature t-1 = 3-1 = 2 Error(oven) (t-1)*r - (t-1)= 6 -2 = 4 Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Error(oven x day) (b-1)* t * r - (b-1) - (t-1)*(b-1) = 9 - 1 -2 = 6 Error(recipe x oven x day) (m-1) * b * t * r - 0 = 36 Total N-1 = 53 18.4 Applied analysis in R Get code from Thursday here. "],["analysis-and-inference-for-a-split-plot-design---part-ii.html", "Day 19 Analysis and inference for a split-plot design - Part II 19.1 Announcements 19.2 Review of our experiment 19.3 ANOVA tables 19.4 Applied analysis in R", " Day 19 Analysis and inference for a split-plot design - Part II July 8th, 2025 19.1 Announcements Homework 3 is due this Friday (July 11). Homework 4 is posted and due next Friday (July 18). Semester project: An example is posted. Wednesday July 23: send project for peer review. Schedule for somewhere between July 21-August 1 (anywhere between 8am-5pm): 15 min presentation + 15 min Q&amp;A. Submit final version of your report by August 1. Include ANOVA, stat model, mock R code, discussion of strengths and weaknesses of the experiment design. 19.2 Review of our experiment Research question: can we include more banana than the original recipe? Will the optimum temperature change depending on that recipe? Treatment structure: 3 \\(\\times\\) 2 factorial, with 3 levels for temperature (250, 400, 500) and 2 levels for recipe (Control, Extra Banana) Design structure: split-plot in an CRD, with temperature in the whole plot. Figure 19.1: Muffin experiment With that treatment structure, the statistical model will always begin with \\(y_{ijk} = \\mu + T_i + R_j +(TR)_{ij}\\), followed by the effects coming from the design structure (random effects). 19.3 ANOVA tables 19.3.1 Split-plot in a CRD Table 19.1: ANOVA without subsampling Table 19.1: Design or Topographical Sources of Variability Source df - Error(oven) t*r-1 = 9-1 = 8 - - Error(recipe x oven), or recipe(oven) (b-1)* t * r = (2-1) * 3 * 3 = 9 Total N-1 = 17 Table 19.1: Treatment Sources of Variability Source df Temperature t-1 = 3-1 = 2 - Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Parallels N-tb = 18 - (3*2) = 12 Total N-1 = 17 Table 19.1: Combined Table of the Sources of Variability Source df Temperature t-1 = 3-1 = 2 Error(oven) t*r-1 - (t-1)= 8 - 2 = 6 Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Error(recipe x oven), or recipe(oven) (b-1)* t * r - (b-1) - (t-1)*(b-1) = 9 - 1 -2 = 6 Total N-1 = 17 19.3.1.1 Implications on inference The treatment mean for the \\(i\\)th temperature and \\(j\\)th banana level is \\(\\mu_{ij} = \\mu + T_i + B_j +(TB)_{ij}\\) and won’t change under different design structures. What may change is the confidence interval around the mean difference. Recall the formula for a CI: \\(\\theta \\pm t_{df, \\frac{\\alpha}{2}} \\cdot se(\\hat{\\theta})\\). #get test t qt(p = .975, df = 6) # df are df error(oven) ## [1] 2.446912 For example, the CI for the differences between means for 250F and 400F \\(\\mu_{1 \\cdot} - \\mu_{2 \\cdot}\\) is \\((\\mu_{1 \\cdot} - \\mu_{2 \\cdot}) \\pm 2.45 \\cdot se(\\widehat{\\mu_{1 \\cdot} - \\mu_{2 \\cdot}})\\) \\(se(\\widehat{\\mu_{1 \\cdot} - \\mu_{2 \\cdot}}) = \\sqrt{\\frac{2 (\\sigma^2_{\\varepsilon} + b \\cdot \\sigma^2_w)}{b \\cdot r}}\\) \\[\\mu_i \\pm 2.45 \\cdot \\sqrt{\\frac{2 (\\sigma^2_{\\varepsilon} + b \\cdot \\sigma^2_w)}{b \\cdot r}}\\] #get test t qt(p = .975, df = 6) # df are df error(recipe x oven), or recipe(oven) ## [1] 2.446912 The CI for the differences between means for normal and high banana \\(\\mu_{\\cdot 1} - \\mu_{\\cdot 2}\\) is \\((\\mu_{\\cdot 1} - \\mu_{\\cdot 2}) \\pm 2.45 \\cdot se(\\widehat{\\mu_{\\cdot 1} - \\mu_{\\cdot 2}})\\) \\(se(\\widehat{\\mu_{\\cdot 1} - \\mu_{\\cdot 2}}) = \\sqrt{\\frac{2 \\sigma^2_{\\varepsilon}}{t \\cdot r}}\\) \\[\\mu_i \\pm 2.45 \\cdot \\sqrt{\\frac{2 \\sigma^2_{\\varepsilon}}{r t}}\\] # libraries library(tidyverse) library(lme4) library(emmeans) # data url &lt;- &quot;https://raw.githubusercontent.com/stat720/summer2025/refs/heads/main/data/muffin_data.csv&quot; muffins &lt;- read.csv(url) muffins$oven_temp &lt;- as.factor(muffins$oven_temp) muffins$rep &lt;- as.factor(muffins$rep) muffins1 &lt;- muffins %&gt;% filter(subsample == 1) # fit model model_subsampling &lt;- lmer(height_cm ~ oven_temp*recipe + (1|rep), data = muffins1) # get variance components sigma2 &lt;- sigma(model_subsampling)^2 sigma2_r &lt;- 0.1683^2 r &lt;- 3 # nr of repetitions t &lt;- 3 # nr of levels for temp b &lt;- 2 # nr of levels for banana recipe # see contrast df and SE emmeans(model_subsampling, ~oven_temp, contr = list(c(1, -1, 0)) ) ## $emmeans ## oven_temp emmean SE df lower.CL upper.CL ## 250 2.30 0.152 6 1.93 2.67 ## 400 2.62 0.152 6 2.24 2.99 ## 500 3.02 0.152 6 2.64 3.39 ## ## Results are averaged over the levels of: recipe ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## c(1, -1, 0) -0.317 0.216 6 -1.469 0.1923 ## ## Results are averaged over the levels of: recipe ## Degrees-of-freedom method: kenward-roger # this should match the SE above sqrt(2*(sigma2 + b*sigma2_r)/(r*b)) ## [1] 0.2155826 # see contrast df and SE emmeans(model_subsampling, ~recipe, contr = list(c(1, -1)) ) ## $emmeans ## recipe emmean SE df lower.CL upper.CL ## B 2.30 0.111 11.3 2.06 2.54 ## C 2.99 0.111 11.3 2.75 3.23 ## ## Results are averaged over the levels of: oven_temp ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## c(1, -1) -0.689 0.136 6 -5.079 0.0023 ## ## Results are averaged over the levels of: oven_temp ## Degrees-of-freedom method: kenward-roger # this should match the SE above sqrt(2*sigma2/(r*t)) ## [1] 0.1356284 19.3.2 Same design as above, with subsampling Table 19.2: ANOVA without subsampling Table 19.2: Design or Topographical Sources of Variability Source df - Error(oven) t*r-1 = 9-1 = 8 - - Error(recipe x oven), or recipe(oven) (b-1)* t * r = (2-1) * 3 * 3 = 9 Error(muffin x recipe x oven), or muffin(recipe x oven) (s-1)* b * t * r = (3-1) * 2 * 3 * 3 = 36 Total N-1 = 53 Table 19.2: Treatment Sources of Variability Source df Temperature t-1 = 3-1 = 2 - Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Parallels N-tb = 54 - (3*2) = 48 Total N-1 = 53 Table 19.2: Combined Table of the Sources of Variability Source df Temperature t-1 = 3-1 = 2 Error(oven) t*r-1 - (t-1)= 8 - 2 = 6 Banana b-1 = 2-1 = 1 TxB (t-1)*(b-1) = 2 Error(recipe x oven), or recipe(oven) (b-1)* t * r - (b-1) - (t-1)*(b-1) = 9 - 1 -2 = 6 Error(muffin x recipe x oven), or muffin(recipe x oven) (s-1)* b * t * r = 36 Total N-1 = 53 19.4 Applied analysis in R Get code here. "],["power-analysis.html", "Day 20 Power analysis 20.1 Announcements 20.2 Review of everything so far 20.3 Power of an experiment 20.4 Reminders", " Day 20 Power analysis July 9th, 2025 20.1 Announcements Today is Argentina’s independence day! 20.2 Review of everything so far Figure 20.1: Mindmap 20.3 Power of an experiment Let’s first review the different types of errors: Figure 20.2: Types of error We normally control \\(\\alpha\\), the probability of doing an error of type I. TO describe our experiment’s ability to detect scientific discoveries, we consider power: the \\(P(\\text{reject } H_0 | H_0 \\text{ false}) = 1- \\beta\\). Very often, research teams consider 0.8 power as the minimum for an experiment. 20.3.1 Power calculations t-test Used to evaluate differences between treatment effects or means. Example testing against zero: \\(t^{\\star} = \\frac{\\hat\\theta - 0}{s.e.(\\hat\\theta)} = \\frac{\\hat\\theta - 0}{s.d.(\\hat\\theta)/\\sqrt{n}}\\) Note the sensibility to sample size. Also, remember the \\(s.e.(\\hat\\theta)\\) may differ depending on the design: \\(s.e.(\\hat\\theta)\\) may depend on \\(\\sigma^2_\\varepsilon\\) only (e.g., CRD, RCBD, mean comparisons between treatment levels of the treatment at the split-plot level), \\(s.e.(\\hat\\theta) = \\sqrt{\\frac{2 \\sigma^2_{\\varepsilon}}{r t}}\\). \\(s.e.(\\hat\\theta)\\) may depend on \\(\\sigma^2_\\varepsilon\\) and \\(\\sigma^2_{whole \\ plot}\\) (e.g., mean comparisons between treatment levels of the treatment at the whole-plot level), \\(s.e.(\\hat\\theta) = \\sqrt{\\frac{2 (\\sigma^2_{\\varepsilon} + b \\cdot \\sigma^2_w)}{b \\cdot r}}\\) To detect a difference \\(\\delta\\): \\[n = \\frac{2\\hat{\\sigma}^2}{\\delta^2}[t_{\\alpha/2, \\nu} + t_{\\beta, \\nu}]^2,\\] where: \\(n\\) is the sample size, \\(\\hat\\sigma^2\\) is the estimate of \\(\\sigma^2\\) based on \\(\\nu\\) degrees of freedom, \\(\\alpha\\) is the type I error rate, \\(\\beta\\) is the type II error rate, And note that: \\(Var(\\delta)=2\\sigma^2/n\\). 20.4 Reminders HW 3 due Friday HW4 is posted Project proposal due dates: July 23, August 1st. "],["power-analysis-ii.html", "Day 21 Power analysis II 21.1 Announcements 21.2 What are simulations? 21.3 Power analysis demonstration", " Day 21 Power analysis II July 10th, 2025 21.1 Announcements HW 3 due tomorrow 21.2 What are simulations? A lot the concepts we’re learning in STAT 720 are based on asymptotic properties, or what would happen, generally speaking. For example, a 95% confidence interval will include the true value only 95% of the times. That means that 5% of the times, it will not. Likewise, a hypothesis test with an \\(\\alpha=0.05\\) will incorrectly reject the null hypothesis 5% \\((=\\alpha)\\) of the times. That hypothesis test will also have an associated \\(\\beta\\), that will depend on factors like sample size and experiment design. If we wish to study how a design would work, we should test it many many times and not just once. Simulation studies are helpful to evaluate how a method performs, generally speaking. 21.2.1 Simulation demo 1. Open the required packages library(tidyverse) # data wrangling &amp; data viz library(lme4) # model fitting library(emmeans) # marginal means library(latex2exp) # math notation for plots 2. Set the “true” values # set the true state b0.truth &lt;- 2 # true intercept value b1.truth &lt;- 4 # true slope value sigma.truth &lt;- 1 # true variance # create predictor x &lt;- seq(1, 60, by = 7) Demo: what happens if we simulate data based on known true values for the parameters, and then try to estimate the parameters again? Most likely, the confidence intervals will contain the true value. Let’s try it once: # generate &quot;fake data&quot; based on the mean and some random error set.seed(22) random_error &lt;- rnorm(length(x), 0, sigma.truth) y &lt;- b0.truth + x*b1.truth + random_error # fit the model to the data m &lt;- lm(y ~ x) # 95% confidence intervals confint(m) ## 2.5 % 97.5 % ## (Intercept) 1.410706 4.592939 ## x 3.936110 4.029237 between(b0.truth, confint(m)[1,1], confint(m)[1,2]) ## [1] TRUE between(b1.truth, confint(m)[2,1], confint(m)[2,2]) ## [1] TRUE 3. Do that 1000 times The test above worked - the 95% CIs included the true values. Based on our knowledge though, we would expect some error rate. Let’s check out what happens if we repeat this 1000 times: hist(b0.hat, xlab = TeX(&quot;$\\\\hat{\\\\beta}_0$&quot;), main = TeX(&quot;Histogram of the different $\\\\hat{\\\\beta}_0$ estimated in the synthetic datasets&quot;)) hist(b1.hat, xlab = TeX(&quot;$\\\\hat{\\\\beta}_1$&quot;), main = TeX(&quot;Histogram of the different $\\\\hat{\\\\beta}_1$ estimated in the synthetic datasets&quot;)) paste0(&quot;In the 1000 simulated scenarios, the confidence interval included the true beta_0 &quot;, 100*mean(b0.hat.calib), &quot;% of the times.&quot;, sep = &quot;&quot;) ## [1] &quot;In the 1000 simulated scenarios, the confidence interval included the true beta_0 95.9% of the times.&quot; paste0(&quot;In the 1000 simulated scenarios, the confidence interval included the true beta_1 &quot;, 100*mean(b1.hat.calib), &quot;% of the times.&quot;, sep = &quot;&quot;) ## [1] &quot;In the 1000 simulated scenarios, the confidence interval included the true beta_1 95.9% of the times.&quot; 21.3 Power analysis demonstration We’ll simulate the design of an experiment. Let’s assume the following: 3 \\(\\times\\) 2 factorial treatment structure, with temperature at 250, 400, and 500 F, and 2 muffin recipes. Treatment means: \\(\\mu_{11}=2\\), \\(\\mu_{21}=2.4\\), \\(\\mu_{31}=2.6\\), \\(\\mu_{12}=2.4\\), \\(\\mu_{22}=2.8\\), \\(\\mu_{32}=3\\). Which means: \\(\\mu_{1\\cdot} - \\mu_{2\\cdot} = -0.4\\), \\(\\mu_{1\\cdot} - \\mu_{2\\cdot} = -0.4\\). Consider these competing designs: CRD split-plot design split-plot design with subsampling All examples below generate 100 designs with the means described above, the designs with 2 or 3 repetitions, and the corresponding variances for each design (e.g., \\(\\sigma^2_\\varepsilon\\), \\(\\sigma^2_{whole plot}\\), etc.). 21.3.1 CRD - 2 reps df_crd &lt;- expand.grid(oven_temp = factor(c(250, 400, 500)), recipe = c(&quot;B&quot;, &quot;C&quot;), rep = 1:2) df_crd &lt;- df_crd %&gt;% mutate(mu = case_when(oven_temp == &quot;250&quot; &amp; recipe == &quot;B&quot; ~ 2, oven_temp == &quot;400&quot; &amp; recipe == &quot;B&quot; ~ 2.4, oven_temp == &quot;500&quot; &amp; recipe == &quot;B&quot; ~ 2.6, oven_temp == &quot;250&quot; &amp; recipe == &quot;C&quot; ~ 2.4, oven_temp == &quot;400&quot; &amp; recipe == &quot;C&quot; ~ 2.8, oven_temp == &quot;500&quot; &amp; recipe == &quot;C&quot; ~ 3)) sigma_epsilon &lt;- 0.26 n_sims &lt;- 100 p_mu.1_mu.2 &lt;- numeric(n_sims) p_mu1._mu2. &lt;- numeric(n_sims) n &lt;- nrow(df_crd) set.seed(42) for (i in 1:n_sims){ df_temp &lt;- df_crd %&gt;% mutate(y = mu + rnorm(n, 0, sigma_epsilon)) m &lt;- lm(y ~ oven_temp * recipe, data = df_temp) p_mu.1_mu.2[i] &lt;- as.data.frame(emmeans(m, ~recipe, contr = list(c(1, -1)))$contrasts)$p.value p_mu1._mu2.[i] &lt;- as.data.frame(emmeans(m, ~oven_temp, contr = list(c(1, -1, 0)))$contrasts)$p.value } hist(p_mu.1_mu.2, xlim = c(0, 1), xlab = TeX(&quot;$\\\\hat{\\\\mu}_{\\\\cdot 1} - \\\\hat{\\\\mu}_{\\\\cdot 2}$&quot;), main = TeX(&quot;p-value for $\\\\hat{\\\\mu}_{\\\\cdot 1} - \\\\hat{\\\\mu}_{\\\\cdot 2}$ under a CRD with 2 reps&quot;)) hist(p_mu1._mu2., xlim = c(0, 1), xlab = TeX(&quot;$\\\\hat{\\\\mu}_{1 \\\\cdot} - \\\\hat{\\\\mu}_{2 \\\\cdot}$&quot;), main = TeX(&quot;p-value for $\\\\hat{\\\\mu}_{1 \\\\cdot} - \\\\hat{\\\\mu}_{2 \\\\cdot}$ under a CRD with 2 reps&quot;)) mean(p_mu.1_mu.2&lt;0.05) # power for .4 difference in recipe ## [1] 0.002 mean(p_mu1._mu2.&lt;0.05) # power for .4 difference in temp ## [1] 0 21.3.2 CRD - 3 reps df_crd &lt;- expand.grid(oven_temp = factor(c(250, 400, 500)), recipe = c(&quot;B&quot;, &quot;C&quot;), rep = 1:3) df_crd &lt;- df_crd %&gt;% mutate(mu = case_when(oven_temp == &quot;250&quot; &amp; recipe == &quot;B&quot; ~ 2, oven_temp == &quot;400&quot; &amp; recipe == &quot;B&quot; ~ 2.4, oven_temp == &quot;500&quot; &amp; recipe == &quot;B&quot; ~ 2.6, oven_temp == &quot;250&quot; &amp; recipe == &quot;C&quot; ~ 2.4, oven_temp == &quot;400&quot; &amp; recipe == &quot;C&quot; ~ 2.8, oven_temp == &quot;500&quot; &amp; recipe == &quot;C&quot; ~ 3)) sigma_epsilon &lt;- 0.26 p_mu.1_mu.2 &lt;- numeric(n_sims) p_mu1._mu2. &lt;- numeric(n_sims) n &lt;- nrow(df_crd) set.seed(42) for (i in 1:n_sims){ df_temp &lt;- df_crd %&gt;% mutate(y = mu + rnorm(n, 0, sigma_epsilon)) m &lt;- lm(y ~ oven_temp * recipe, data = df_temp) p_mu.1_mu.2[i] &lt;- as.data.frame(emmeans(m, ~recipe, contr = list(c(1, -1)))$contrasts)$p.value p_mu1._mu2.[i] &lt;- as.data.frame(emmeans(m, ~oven_temp, contr = list(c(1, -1, 0)))$contrasts)$p.value } hist(p_mu.1_mu.2, xlim = c(0, 1), xlab = TeX(&quot;$\\\\hat{\\\\mu}_{\\\\cdot 1} - \\\\hat{\\\\mu}_{\\\\cdot 2}$&quot;), main = TeX(&quot;p-value for $\\\\hat{\\\\mu}_{\\\\cdot 1} - \\\\hat{\\\\mu}_{\\\\cdot 2}$ under a CRD with 3 reps&quot;)) hist(p_mu1._mu2., xlim = c(0, 1), xlab = TeX(&quot;$\\\\hat{\\\\mu}_{1 \\\\cdot} - \\\\hat{\\\\mu}_{2 \\\\cdot}$&quot;), main = TeX(&quot;p-value for $\\\\hat{\\\\mu}_{1 \\\\cdot} - \\\\hat{\\\\mu}_{2 \\\\cdot}$ under a CRD with 3 reps&quot;)) mean(p_mu.1_mu.2&lt;0.05) # power for .4 difference in recipe ## [1] 0.81 mean(p_mu1._mu2.&lt;0.05) # power for .4 difference in temperature ## [1] 0.71 21.3.3 Split-plot - 2 reps df_splitplot &lt;- expand.grid(oven_temp = factor(c(250, 400, 500)), recipe = c(&quot;B&quot;, &quot;C&quot;), rep = 1:2) %&gt;% mutate(wp_id = as.numeric(as.factor(paste(oven_temp, rep)))) df_splitplot &lt;- df_splitplot %&gt;% mutate(mu = case_when(oven_temp == &quot;250&quot; &amp; recipe == &quot;B&quot; ~ 2, oven_temp == &quot;400&quot; &amp; recipe == &quot;B&quot; ~ 2.4, oven_temp == &quot;500&quot; &amp; recipe == &quot;B&quot; ~ 2.6, oven_temp == &quot;250&quot; &amp; recipe == &quot;C&quot; ~ 2.4, oven_temp == &quot;400&quot; &amp; recipe == &quot;C&quot; ~ 2.8, oven_temp == &quot;500&quot; &amp; recipe == &quot;C&quot; ~ 3)) sigma_epsilon &lt;- 0.26 sigma_oven &lt;- .12 n &lt;- nrow(df_splitplot) n_wp &lt;- n_distinct(df_splitplot$oven_temp) * n_distinct(df_splitplot$rep) p_mu.1_mu.2 &lt;- numeric(n_sims) p_mu1._mu2. &lt;- numeric(n_sims) set.seed(42) for (i in 1:n_sims){ oven_re &lt;- rnorm(n_wp, 0, sigma_oven) df_temp &lt;- df_splitplot %&gt;% mutate(y = mu + oven_re[wp_id] + rnorm(n, 0, sigma_epsilon) ) m &lt;- lmer(y ~ oven_temp * recipe + (1|oven_temp:rep), data = df_temp) p_mu.1_mu.2[i] &lt;- as.data.frame(emmeans(m, ~recipe, contr = list(c(1, -1)))$contrasts)$p.value p_mu1._mu2.[i] &lt;- as.data.frame(emmeans(m, ~oven_temp, contr = list(c(1, -1, 0)))$contrasts)$p.value } hist(p_mu.1_mu.2, xlim = c(0, 1), xlab = TeX(&quot;$\\\\hat{\\\\mu}_{\\\\cdot 1} - \\\\hat{\\\\mu}_{\\\\cdot 2}$&quot;), main = TeX(&quot;p-value for $\\\\hat{\\\\mu}_{\\\\cdot 1} - \\\\hat{\\\\mu}_{\\\\cdot 2}$ under a split-plot design with 2 reps&quot;)) hist(p_mu1._mu2., xlim = c(0, 1), xlab = TeX(&quot;$\\\\hat{\\\\mu}_{1 \\\\cdot} - \\\\hat{\\\\mu}_{2 \\\\cdot}$&quot;), main = TeX(&quot;p-value for $\\\\hat{\\\\mu}_{1 \\\\cdot} - \\\\hat{\\\\mu}_{2 \\\\cdot}$ under a split-plot design with 2 reps&quot;)) mean(p_mu.1_mu.2&lt;0.05) # power for .4 difference at the split plot ## [1] 0.56 mean(p_mu1._mu2.&lt;0.05) # power for .4 difference at the whole plot ## [1] 0.11 21.3.4 Split-plot - 3 reps df_splitplot &lt;- expand.grid(oven_temp = factor(c(250, 400, 500)), recipe = c(&quot;B&quot;, &quot;C&quot;), rep = 1:3) %&gt;% mutate(wp_id = as.numeric(as.factor(paste(oven_temp, rep)))) df_splitplot &lt;- df_splitplot %&gt;% mutate(mu = case_when(oven_temp == &quot;250&quot; &amp; recipe == &quot;B&quot; ~ 2, oven_temp == &quot;400&quot; &amp; recipe == &quot;B&quot; ~ 2.4, oven_temp == &quot;500&quot; &amp; recipe == &quot;B&quot; ~ 2.6, oven_temp == &quot;250&quot; &amp; recipe == &quot;C&quot; ~ 2.4, oven_temp == &quot;400&quot; &amp; recipe == &quot;C&quot; ~ 2.8, oven_temp == &quot;500&quot; &amp; recipe == &quot;C&quot; ~ 3)) sigma_epsilon &lt;- 0.26 sigma_oven &lt;- .12 n &lt;- nrow(df_splitplot) n_wp &lt;- n_distinct(df_splitplot$oven_temp) * n_distinct(df_splitplot$rep) p_mu.1_mu.2 &lt;- numeric(n_sims) p_mu1._mu2. &lt;- numeric(n_sims) set.seed(42) for (i in 1:n_sims){ oven_re &lt;- rnorm(n_wp, 0, sigma_oven) df_temp &lt;- df_splitplot %&gt;% mutate(y = mu + oven_re[wp_id] + rnorm(n, 0, sigma_epsilon) ) m &lt;- lmer(y ~ oven_temp * recipe + (1|oven_temp:rep), data = df_temp) p_mu.1_mu.2[i] &lt;- as.data.frame(emmeans(m, ~recipe, contr = list(c(1, -1)))$contrasts)$p.value p_mu1._mu2.[i] &lt;- as.data.frame(emmeans(m, ~oven_temp, contr = list(c(1, -1, 0)))$contrasts)$p.value } hist(p_mu.1_mu.2, xlim = c(0, 1), xlab = TeX(&quot;$\\\\hat{\\\\mu}_{\\\\cdot 1} - \\\\hat{\\\\mu}_{\\\\cdot 2}$&quot;), main = TeX(&quot;p-value for $\\\\hat{\\\\mu}_{\\\\cdot 1} - \\\\hat{\\\\mu}_{\\\\cdot 2}$ under a split-plot design with 3 reps&quot;)) hist(p_mu1._mu2., xlab = TeX(&quot;$\\\\hat{\\\\mu}_{1 \\\\cdot} - \\\\hat{\\\\mu}_{2 \\\\cdot}$&quot;), main = TeX(&quot;p-value for $\\\\hat{\\\\mu}_{1 \\\\cdot} - \\\\hat{\\\\mu}_{2 \\\\cdot}$ under a split-plot design with 3 reps&quot;)) mean(p_mu.1_mu.2&lt;0.05) # power for .4 difference at the split plot ## [1] 0.77 mean(p_mu1._mu2.&lt;0.05) # power for .4 difference at the whole plot ## [1] 0.38 21.3.5 Split-plot - 2 reps + subsampling df_splitplot_subsample &lt;- expand.grid(oven_temp = factor(c(250, 400, 500)), recipe = c(&quot;B&quot;, &quot;C&quot;), rep = 1:2, subsample = 1:3) %&gt;% mutate(wp_id = as.numeric(as.factor(paste(rep, oven_temp))), muffin_id = as.numeric(as.factor(paste(rep, oven_temp, recipe)))) df_splitplot_subsample &lt;- df_splitplot_subsample %&gt;% mutate(mu = case_when(oven_temp == &quot;250&quot; &amp; recipe == &quot;B&quot; ~ 2, oven_temp == &quot;400&quot; &amp; recipe == &quot;B&quot; ~ 2.4, oven_temp == &quot;500&quot; &amp; recipe == &quot;B&quot; ~ 2.6, oven_temp == &quot;250&quot; &amp; recipe == &quot;C&quot; ~ 2.4, oven_temp == &quot;400&quot; &amp; recipe == &quot;C&quot; ~ 2.8, oven_temp == &quot;500&quot; &amp; recipe == &quot;C&quot; ~ 3)) sigma_epsilon &lt;- 0.26 sigma_oven &lt;- .12 sigma_muffins &lt;- .1 n &lt;- nrow(df_splitplot) n_wp &lt;- n_distinct(df_splitplot$oven_temp) * n_distinct(df_splitplot$rep) n_sp &lt;- n_distinct(df_splitplot$oven_temp) * n_distinct(df_splitplot$rep) * n_distinct(df_splitplot$recipe) p_mu.1_mu.2 &lt;- numeric(n_sims) p_mu1._mu2. &lt;- numeric(n_sims) set.seed(42) for (i in 1:n_sims){ oven_re &lt;- rnorm(n_wp, 0, sigma_oven) muffins_re &lt;- rnorm(n_sp, 0, sigma_muffins) df_temp &lt;- df_splitplot_subsample %&gt;% mutate(y = mu + oven_re[wp_id] + muffins_re[muffin_id] + rnorm(n, 0, sigma_epsilon) ) m &lt;- lmer(y ~ oven_temp * recipe + (1|oven_temp:rep/recipe), data = df_temp) p_mu.1_mu.2[i] &lt;- as.data.frame(emmeans(m, ~recipe, contr = list(c(1, -1)))$contrasts)$p.value p_mu1._mu2.[i] &lt;- as.data.frame(emmeans(m, ~oven_temp, contr = list(c(1, -1, 0)))$contrasts)$p.value } hist(p_mu.1_mu.2, xlim = c(0, 1), xlab = TeX(&quot;$\\\\hat{\\\\mu}_{\\\\cdot 1} - \\\\hat{\\\\mu}_{\\\\cdot 2}$&quot;), main = TeX(&quot;p-value for $\\\\hat{\\\\mu}_{\\\\cdot 1} - \\\\hat{\\\\mu}_{\\\\cdot 2}$ under a split-plot design with 2 reps and subsampling&quot;)) hist(p_mu1._mu2., xlim = c(0, 1), xlab = TeX(&quot;$\\\\hat{\\\\mu}_{1 \\\\cdot} - \\\\hat{\\\\mu}_{2 \\\\cdot}$&quot;), main = TeX(&quot;p-value for $\\\\hat{\\\\mu}_{1 \\\\cdot} - \\\\hat{\\\\mu}_{2 \\\\cdot}$ under a split-plot design with 2 reps and subsampling&quot;)) mean(p_mu.1_mu.2&lt;0.05) # power for .4 difference at the split plot ## [1] 0.79 mean(p_mu1._mu2.&lt;0.05) # power for .4 difference at the whole plot ## [1] 0.41 21.3.6 Split-plot - 3 reps + subsampling df_splitplot_subsample &lt;- expand.grid(oven_temp = factor(c(250, 400, 500)), recipe = c(&quot;B&quot;, &quot;C&quot;), rep = 1:4, subsample = 1:3) %&gt;% mutate(wp_id = as.numeric(as.factor(paste(rep, oven_temp))), muffin_id = as.numeric(as.factor(paste(rep, oven_temp, recipe)))) df_splitplot_subsample &lt;- df_splitplot_subsample %&gt;% mutate(mu = case_when(oven_temp == &quot;250&quot; &amp; recipe == &quot;B&quot; ~ 2, oven_temp == &quot;400&quot; &amp; recipe == &quot;B&quot; ~ 2.4, oven_temp == &quot;500&quot; &amp; recipe == &quot;B&quot; ~ 2.6, oven_temp == &quot;250&quot; &amp; recipe == &quot;C&quot; ~ 2.4, oven_temp == &quot;400&quot; &amp; recipe == &quot;C&quot; ~ 2.8, oven_temp == &quot;500&quot; &amp; recipe == &quot;C&quot; ~ 3)) sigma_epsilon &lt;- 0.26 sigma_oven &lt;- .12 sigma_muffins &lt;- .1 n &lt;- nrow(df_splitplot) n_wp &lt;- n_distinct(df_splitplot$oven_temp) * n_distinct(df_splitplot$rep) n_sp &lt;- n_distinct(df_splitplot$oven_temp) * n_distinct(df_splitplot$rep) * n_distinct(df_splitplot$recipe) p_mu.1_mu.2 &lt;- numeric(n_sims) p_mu1._mu2. &lt;- numeric(n_sims) set.seed(3) for (i in 1:n_sims){ oven_re &lt;- rnorm(n_wp, 0, sigma_oven) muffins_re &lt;- rnorm(n_sp, 0, sigma_muffins) df_temp &lt;- df_splitplot_subsample %&gt;% mutate(y = mu + oven_re[wp_id] + muffins_re[muffin_id] + rnorm(n, 0, sigma_epsilon) ) m &lt;- lmer(y ~ oven_temp * recipe + (1|oven_temp:rep/recipe), data = df_temp) p_mu.1_mu.2[i] &lt;- as.data.frame(emmeans(m, ~recipe, contr = list(c(1, -1)))$contrasts)$p.value p_mu1._mu2.[i] &lt;- as.data.frame(emmeans(m, ~oven_temp, contr = list(c(1, -1, 0)))$contrasts)$p.value } hist(p_mu.1_mu.2, xlim = c(0, 1), xlab = TeX(&quot;$\\\\hat{\\\\mu}_{\\\\cdot 1} - \\\\hat{\\\\mu}_{\\\\cdot 2}$&quot;), main = TeX(&quot;p-value for $\\\\hat{\\\\mu}_{\\\\cdot 1} - \\\\hat{\\\\mu}_{\\\\cdot 2}$ under a split-plot design with 3 reps and subsampling&quot;)) hist(p_mu1._mu2., xlim = c(0, 1), xlab = TeX(&quot;$\\\\hat{\\\\mu}_{1 \\\\cdot} - \\\\hat{\\\\mu}_{2 \\\\cdot}$&quot;), main = TeX(&quot;p-value for $\\\\hat{\\\\mu}_{1 \\\\cdot} - \\\\hat{\\\\mu}_{2 \\\\cdot}$ under a split-plot design with 3 reps and subsampling&quot;)) mean(p_mu.1_mu.2&lt;0.05) # power for .4 difference at the split plot ## [1] 0.95 mean(p_mu1._mu2.&lt;0.05) # power for .4 difference at the whole plot ## [1] 0.71 "],["review-friday.html", "Day 22 Review Friday 22.1 Announcements", " Day 22 Review Friday July 11th, 2025 22.1 Announcements HW 3 due today "],["randomized-complete-block-designs.html", "Day 23 Randomized complete block designs 23.1 Announcements 23.2 Randomized complete block designs 23.3 Blocks - fixed or random? 23.4 Incomplete block designs", " Day 23 Randomized complete block designs July 14th, 2025 23.1 Announcements HW 4 due Friday Topics for the week of July 21 - survey 23.2 Randomized complete block designs Blocks are groups of approximately similar experimental units. Data generated by blocked designs may be analyzed as \\(y_{ij} = \\mu +\\tau_i + b_j + \\varepsilon_{ij}\\), where \\(y_{ij}\\) is the observed response for the \\(i\\)th treatment in the \\(j\\)th block, \\(\\tau_i\\) is the effect of the \\(i\\)th treatment (\\(i = 1, 2, ..., t\\)), \\(b_j\\) is the effect of the \\(j\\)th block, \\(\\varepsilon_{ij}\\) is the residual of the \\(i\\)th treatment in the \\(j\\)th block. Most RCBDs have \\(k = t\\), where \\(k\\) is the number of experimental units per block, and \\(t\\) is the number of treatments. 23.3 Blocks - fixed or random? The assumptions behind \\(b_j\\) vary: Fundamentals of mixed models Assumption behind blocks as fixed effects: there is a ‘true’ block effect out there. Assumption behind blocks as random effects: Confidence intervals of the means differ depending on the model: Blocks as fixed: \\(\\hat\\mu \\pm t \\cdot \\sqrt{\\frac{\\sigma_{\\varepsilon}^2 }{b}}\\) Blocks as random: \\(\\hat\\mu \\pm t \\cdot \\sqrt{\\frac{\\sigma_{\\varepsilon}^2 + \\sigma^2_{b}}{b}}\\) Confidence intervals of the means differences don’t differ depending on the model: Blocks as fixed/as random: \\(\\hat\\mu \\pm t \\cdot \\sqrt{\\frac{2 \\sigma_{\\varepsilon}^2 }{n}}\\) More on the differences between blocks as fixed vs random: Dixon 2016. 23.3.1 Applied design and analysis of an RCBD An experiment was run in South Australia to compare for 107 different wheat varieties, in a randomized complete block design with 3 blocks. Get R code here. Table 23.1: ANOVA table for the RCBD Source df Blocks b-1 = 2 Genotype g-1 = 106 Error N-b-g+1 = 221 Total N-1 = 329 library(tidyverse) library(agridat) library(lme4) library(emmeans) dat_wheat_rcbd &lt;- agridat::gilmour.serpentine dat_wheat_rcbd %&gt;% ggplot(aes(col, row))+ geom_tile(aes(fill = rep)) m_block.as.fixed &lt;- lm(yield ~ gen + rep, data = dat_wheat_rcbd) m_block.as.random &lt;- lme4::lmer(yield ~ gen + (1|rep), data = dat_wheat_rcbd) emmeans(m_block.as.fixed, ~gen)[1:10,] ## gen emmean SE df lower.CL upper.CL ## (WWH*MM)*WR* 709 66.7 221 577 841 ## (WqKPWmH*3Ag 733 66.7 221 602 865 ## AMERY 616 66.7 221 484 747 ## ANGAS 576 66.7 221 445 708 ## AROONA 555 66.7 221 424 687 ## BATAVIA 534 66.7 221 402 665 ## BD231 639 66.7 221 507 770 ## BEULAH 535 66.7 221 404 667 ## BLADE 439 66.7 221 307 571 ## BT_SCHOMBURG 660 66.7 221 528 792 ## ## Results are averaged over the levels of: rep ## Confidence level used: 0.95 emmeans(m_block.as.random, ~gen)[1:10,] ## gen emmean SE df lower.CL upper.CL ## (WWH*MM)*WR* 709 93.3 8.16 495 923 ## (WqKPWmH*3Ag 733 93.3 8.16 519 948 ## AMERY 616 93.3 8.16 401 830 ## ANGAS 576 93.3 8.16 362 791 ## AROONA 555 93.3 8.16 341 770 ## BATAVIA 534 93.3 8.16 319 748 ## BD231 639 93.3 8.16 424 853 ## BEULAH 535 93.3 8.16 321 750 ## BLADE 439 93.3 8.16 225 653 ## BT_SCHOMBURG 660 93.3 8.16 446 874 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 emmeans(m_block.as.fixed, ~gen, contr = list(c(1, -1, rep(0, n_distinct(dat_wheat_rcbd$gen)-2))))$contr ## contrast estimate SE df t.ratio p.value ## c(1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -24.3 94.4 221 -0.258 0.7968 ## ## Results are averaged over the levels of: rep emmeans(m_block.as.random, ~gen, contr = list(c(1, -1, rep(0, n_distinct(dat_wheat_rcbd$gen)-2))))$contr ## contrast estimate SE df t.ratio p.value ## c(1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -24.3 94.4 221 -0.258 0.7968 ## ## Degrees-of-freedom method: kenward-roger bind_rows(as.data.frame(emmeans(m_block.as.fixed, ~gen)[1:10,]) %&gt;% transmute(gen, emmean, SE, blocks = &quot;fixed&quot;), as.data.frame(emmeans(m_block.as.random, ~gen)[1:10,]) %&gt;% transmute(gen, emmean, SE, blocks = &quot;random&quot;)) %&gt;% ggplot(aes(gen, emmean))+ geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE, color = blocks), width = .1, position = position_dodge(.1))+ geom_point()+ scale_color_manual(values = c(&quot;tomato&quot;, &quot;#1282A2&quot;))+ theme(axis.text = element_text(angle = 30))+ labs(title = &quot;Difference in Mean SE between fixed blocks and random blocks analyses&quot;) bind_rows(as.data.frame(emmeans(m_block.as.fixed, ~gen, contr = list(c(1, -1, rep(0, n_distinct(dat_wheat_rcbd$gen)-2)), c(1, 0, -1, rep(0, n_distinct(dat_wheat_rcbd$gen)-3))))$contr) %&gt;% transmute(estimate, SE, blocks = &quot;fixed&quot;, contrast = c(&quot;t1 vs. t2&quot;, &quot;t1 vs. t3&quot;)), as.data.frame(emmeans(m_block.as.random, ~gen, contr = list(c(1, -1, rep(0, n_distinct(dat_wheat_rcbd$gen)-2)), c(1, 0, -1, rep(0, n_distinct(dat_wheat_rcbd$gen)-3))))$contr) %&gt;% transmute(estimate, SE, blocks = &quot;random&quot;, contrast = c(&quot;t1 vs. t2&quot;, &quot;t1 vs. t3&quot;))) %&gt;% ggplot(aes(contrast, estimate))+ geom_errorbar(aes(ymin = estimate - SE, ymax = estimate + SE, color = blocks), width = 0.05, position = position_dodge(.05))+ geom_point()+ scale_color_manual(values = c(&quot;tomato&quot;, &quot;#1282A2&quot;))+ theme(axis.text = element_text(angle = 30))+ labs(title = &quot;Difference in Contrast SE between fixed blocks and random blocks analyses&quot;) Mean standard error, blocks as fixed: \\(\\sqrt{\\frac{\\sigma_{\\varepsilon}^2 }{b}}\\) n_blocks &lt;- n_distinct(dat_wheat_rcbd$rep) sigma2_blocks.as.fixed &lt;- sigma(m_block.as.fixed)^2 sqrt(sigma2_blocks.as.fixed / n_blocks) ## [1] 66.73095 Blocks as random: \\(\\hat\\mu \\pm t \\cdot \\sqrt{\\frac{\\sigma_{\\varepsilon}^2 + \\sigma^2_{b}}{b}}\\) sigma2_blocks.as.random &lt;- sigma(m_block.as.random)^2 sigma2b_blocks.as.random &lt;- as.data.frame(VarCorr(m_block.as.random))[1, &#39;sdcor&#39;]^2 sqrt((sigma2b_blocks.as.random + sigma2_blocks.as.random) / n_blocks) ## [1] 93.26549 23.4 Incomplete block designs Same model as before: \\(y_{ij} = \\mu +\\tau_i + b_j + \\varepsilon_{ij}\\), where \\(y_{ij}\\) is the observed response for the \\(i\\)th treatment in the \\(j\\)th block, \\(\\tau_i\\) is the effect of the \\(i\\)th treatment (\\(i = 1, 2, ..., t\\)), \\(b_j\\) is the effect of the \\(j\\)th block, \\(\\varepsilon_{ij}\\) is the residual of the \\(i\\)th treatment in the \\(j\\)th block. In IBDs, \\(k &lt; t\\), where \\(k\\) is the number of experimental units per block, and \\(t\\) is the number of treatments. Better interblock information recovery with random \\(b_j\\). 23.4.1 Applied design and analysis of a Balanced IBD The data below correspond to a study comparing soybean genotypes in a balanced incomplete block designed experiment. Table 23.2: ANOVA table for the soybean BIBD Source df Blocks b-1 = 30 Genotype g-1 = 30 Error N-b-g+1 = 125 Total N-1 = 185 dat_soy_ibd &lt;- agridat::weiss.incblock dat_soy_ibd %&gt;% ggplot(aes(col, row))+ geom_tile(aes(fill = block)) m_block.as.fixed &lt;- lm(yield ~ gen + block, data = dat_soy_ibd) m_block.as.random &lt;- lme4::lmer(yield ~ gen + (1|block), data = dat_soy_ibd) emmeans(m_block.as.fixed, ~gen)[1:10,] ## gen emmean SE df lower.CL upper.CL ## G01 24.6 0.831 125 22.9 26.2 ## G02 26.9 0.831 125 25.3 28.6 ## G03 32.6 0.831 125 31.0 34.3 ## G04 27.0 0.831 125 25.3 28.6 ## G05 26.0 0.831 125 24.4 27.7 ## G06 32.0 0.831 125 30.3 33.6 ## G07 24.2 0.831 125 22.5 25.8 ## G08 27.6 0.831 125 26.0 29.3 ## G09 29.3 0.831 125 27.6 30.9 ## G10 24.4 0.831 125 22.8 26.1 ## ## Results are averaged over the levels of: block ## Confidence level used: 0.95 emmeans(m_block.as.random, ~gen)[1:10,] ## gen emmean SE df lower.CL upper.CL ## G01 24.6 0.923 153 22.7 26.4 ## G02 27.0 0.923 153 25.2 28.8 ## G03 32.6 0.923 153 30.8 34.4 ## G04 26.9 0.923 153 25.1 28.8 ## G05 26.2 0.923 153 24.4 28.0 ## G06 32.0 0.923 153 30.1 33.8 ## G07 24.2 0.923 153 22.3 26.0 ## G08 27.6 0.923 153 25.8 29.4 ## G09 29.4 0.923 153 27.6 31.2 ## G10 24.5 0.923 153 22.7 26.4 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 emmeans(m_block.as.fixed, ~gen, contr = list(c(1, -1, rep(0, n_distinct(dat_soy_ibd$gen)-2))))$contr ## contrast estimate SE df t.ratio p.value ## c(1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2.34 1.18 125 -1.982 0.0496 ## ## Results are averaged over the levels of: block emmeans(m_block.as.random, ~gen, contr = list(c(1, -1, rep(0, n_distinct(dat_soy_ibd$gen)-2))))$contr ## contrast estimate SE df t.ratio p.value ## c(1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -2.4 1.17 129 -2.054 0.0420 ## ## Degrees-of-freedom method: kenward-roger "],["split-plot-designs.html", "Day 24 Split plot designs 24.1 Announcements 24.2 Split plot designs", " Day 24 Split plot designs July 15th, 2025 24.1 Announcements HW 4 due Friday 24.2 Split plot designs Blocks are groups of approximately similar experimental units. Data generated by blocked designs may be analyzed as \\(y_{ij} = \\mu +\\tau_i + b_j + \\varepsilon_{ij}\\), where \\(y_{ij}\\) is the observed response for the \\(i\\)th treatment in the \\(j\\)th block, \\(\\tau_i\\) is the effect of the \\(i\\)th treatment, \\(b_j\\) is the effect of the \\(j\\)th block, \\(\\varepsilon_{ij}\\) is the residual of the \\(i\\)th treatment in the \\(j\\)th block. "],["semester-project.html", "Day 25 Semester Project 25.1 Learning objectives 25.2 Partial deadlines", " Day 25 Semester Project Semester projects may deal with any topic that interests you [the student], as long as it is approved by the instructor. Broadly, projects are expected to identify a research problem and develop a designed experiment that is appropriate for solving that problem. Projects consist of a manuscript and a tutorial that describes the research problem, the experiment design and the treatment design. 25.1 Learning objectives Be able to identify an experiment design that is appropriate for answering a given research question and discuss the strengths and weaknesses of that design for answering the question. Be able to write the materials and methods section of a paper/thesis, including the statistical model that corresponds the experiment design. 25.2 Partial deadlines 25.2.1 Project proposal - Due Friday June 20 at 2pm CT Write a page-long project proposal that states your research problem and the objective of your project. An example of an appropriate project proposal can be found here. 25.2.2 Written report - Due Wednesday July 23 at 2pm CT for peer review Submit a manuscript including: Introduction (background &amp; justification of the problem) Methods, including: A complete ANOVA table describing the degrees of freedom associated to mean comparisons, A clear and complete description of the statistical model, R code that would be implemented to fit that model, Expected results Discussion of strengths and weaknesses of the experiment design (e.g., compare an RCBD with a split-plot design) 25.2.3 Oral presentation - Somewhere between July 21 - August 1 Prepare a 15 minute presentation of the core aspects of your project. Presentations should include at least: Motivation Methods, including a clear and complete description of the statistical model and code Discussion of strengths and weaknesses 25.2.4 Practical reads Casler, M.D. (2015), Fundamentals of Experimental Design: Guidelines for Designing Successful Experiments. Agronomy Journal, 107: 692-705. https://doi.org/10.2134/agronj2013.0114 Casler, M.D. (2018). Power and Replication—Designing Powerful Experiments. In Applied Statistics in Agricultural, Biological, and Environmental Sciences (eds B. Glaz and K.M. Yeater). https://doi.org/10.2134/appliedstatistics.2015.0075.c4 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
